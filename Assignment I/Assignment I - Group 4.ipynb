{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0v2mOU5X6Ls"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "  <h1><font size=6>Assignment 1</font></h1>\n",
        "\n",
        "  <u>Group members:</u><br>\n",
        "  - Ariel Hedvat<br>\n",
        "  - Shiraz Israeli<br>\n",
        "  - Yuval Bakirov<br>\n",
        "  - Eitan Bakirov\n",
        "\n",
        "<br>\n",
        "\n",
        "In this project we are aiming to build an accurate model to predict daily bicycle rental demand using the provided bike sharing dataset. By analyzing the data and testing different modeling techniques, we will develop a robust model optimized to forecast the rental count metric on a held-out test set.<br>\n",
        "The goal is to create a reliable demand prediction model for bike sharing operations.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-LkfVatUx3l"
      },
      "source": [
        "\n",
        "\n",
        "*   כדאי להשתמש במודלים מבוססי עצים: xgboost, random forest\n",
        "*   להעלות את המחברת לגיט האב ולשים את הקישור בתיבת הגשה וגם את הסי-אס-וי\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KQAH9j-qoZw"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "  <h1><font size=5>Data</font></h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeDU9j_8rAD1"
      },
      "source": [
        "`train.csv` and `test.csv` - contain information on bike rentals, including the timestamp, seasonal indicators, holiday and working day flags, weather conditions, temperature metrics, humidity, windspeed, pollution, sunlight, traffic, and the count of bikes rented at each recorded time.<br>\n",
        "Target variable to predict is \"count\" (Label).<br><br>\n",
        "\n",
        "\n",
        "למחוק לפני הגשה:<br>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<u>datetime</u> - time of rental<br>\n",
        "<u>season</u> - (1:winter, 2:spring, 3:summer, 4:fall)<br>\n",
        "<u>holiday</u> - (Is it a bank holiday? If so: 1, else 0)<br>\n",
        "<u>workingday</u> - (Is it a working day? If so: 1, else 0)<br>\n",
        "<u>weather</u>\n",
        "- 1: Clear, Few clouds, Partly cloudy, Partly cloudy<br>\n",
        "- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist<br>\n",
        "- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>\n",
        "- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
        "\n",
        "<u>temp</u> - temperature <br>\n",
        "<u>atemp</u> - average temperature <br>\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfFFUOOZo51l"
      },
      "source": [
        "<div class=\"markdown-google-sans\">\n",
        "  <h1><font size=5>Table of Contents</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "9HArAhUdavbe"
      },
      "source": [
        ">[Import Libraries](#scrollTo=-_VBJ0JlYLKI)\n",
        "\n",
        ">[Loading the data](#scrollTo=kXwOL606ZfUN)\n",
        "\n",
        ">[EDA - Exploring Data Analysis](#scrollTo=P8cTl07AZgyg)\n",
        "\n",
        ">>[Date time Features - Time Series](#scrollTo=W55ei0QSyPJ7)\n",
        "\n",
        ">>[Features Analysis](#scrollTo=kMEpr4k3j2G6)\n",
        "\n",
        ">>>[Categorical Features distribution](#scrollTo=jwF74W59qwf_)\n",
        "\n",
        ">>>[Numerical Features Distribution](#scrollTo=MBRxfPjgrXVc)\n",
        "\n",
        ">>[Label Analysis](#scrollTo=bCeLkOkokIWH)\n",
        "\n",
        ">>[Correlation](#scrollTo=-OtGfcjyFnSI)\n",
        "\n",
        ">>[Missing Values](#scrollTo=G15IFI5TrScB)\n",
        "\n",
        ">>[Outliers Visualization](#scrollTo=rot7m6iisI_v)\n",
        "\n",
        ">[Preprocessing](#scrollTo=E_OOmtAYZnm7)\n",
        "\n",
        ">>[Add / Remove Features](#scrollTo=96lau3rN1OpT)\n",
        "\n",
        ">>[Handling Categorial Features](#scrollTo=xtPSG9CixFck)\n",
        "\n",
        ">>[Outliers](#scrollTo=zxs7JyHhxIoa)\n",
        "\n",
        ">>[Data Normalizing](#scrollTo=NiFHiz6MxXGT)\n",
        "\n",
        ">>[PCA - Dimensionality Reduction](#scrollTo=iaFEPiItxaqq)\n",
        "\n",
        ">>[Final Preprocessing Function](#scrollTo=6TXc09TPPJe0)\n",
        "\n",
        ">[Modelling](#scrollTo=JTkmK9sKe32b)\n",
        "\n",
        ">>[Random forest](#scrollTo=luG0BykVe32i)\n",
        "\n",
        ">>[Linear Regression](#scrollTo=U_CfSlB8bEqd)\n",
        "\n",
        ">>[XGBoost](#scrollTo=nxYuD62xdzMy)\n",
        "\n",
        ">>[Run all models](#scrollTo=Jg2jz3-de32k)\n",
        "\n",
        ">[Evaluating on validation set](#scrollTo=qbvc7vxiZ5fC)\n",
        "\n",
        ">>>[Overall comparison](#scrollTo=v-dUZ0sAcp9-)\n",
        "\n",
        ">>[Feature selection & importance](#scrollTo=Y7M2ZqsX6Ofl)\n",
        "\n",
        ">>>>[Tree based methods](#scrollTo=jl-WQpBxG49h)\n",
        "\n",
        ">>>>[Classical regression methods](#scrollTo=eqYG1lEsG8Aa)\n",
        "\n",
        ">>>>[Comparison](#scrollTo=aFVaV87mICFL)\n",
        "\n",
        ">>[Add/Remove Features to Improve Results](#scrollTo=GE5x3tkL9IwM)\n",
        "\n",
        ">>[Model improvement](#scrollTo=RGzmhm996-4c)\n",
        "\n",
        ">[Prediction - Runing on Test](#scrollTo=dE4SF0VdaENB)\n",
        "\n",
        ">[Output](#scrollTo=IJrSf6hnWnCJ)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_VBJ0JlYLKI"
      },
      "source": [
        "#  Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XRyEMuKdYN_8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn as skl\n",
        "import seaborn as sns\n",
        "import os\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXwOL606ZfUN"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SPuyJ3qXZgWY"
      },
      "outputs": [],
      "source": [
        "# Data Loading for Train\n",
        "url = 'https://raw.githubusercontent.com/ariel-hedvat/AdvancedMLDLCourseAssignments/main/Assignment%20I/train.csv'\n",
        "full_train_data = pd.read_csv(url)\n",
        "\n",
        "train_with_labels = full_train_data.copy()\n",
        "train_data = full_train_data.drop('count', axis=1).copy()\n",
        "train_labels = full_train_data['count'].copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading for Test\n",
        "url_test = 'https://raw.githubusercontent.com/ariel-hedvat/AdvancedMLDLCourseAssignments/main/Assignment%20I/test.csv'\n",
        "full_test_data = pd.read_csv(url_test)\n",
        "\n",
        "test_with_labels = full_test_data.copy()\n",
        "test_data = full_test_data.drop('count', axis=1).copy()\n",
        "test_labels = full_test_data['count'].copy()"
      ],
      "metadata": {
        "id": "z1qQpQAz6T7E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8cTl07AZgyg"
      },
      "source": [
        "# **EDA - Exploring Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghhb5vUZgvjv"
      },
      "source": [
        "A glimpse of the data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "4yhKnP5_ZnTi",
        "outputId": "e3ebbd45-73a1-414e-d364-0220128f4b38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 datetime  season  holiday  workingday  weather   temp  \\\n",
              "0     2011-07-11 00:00:00       3        0           1        1  28.70   \n",
              "1     2012-05-18 22:00:00       2        0           1        1  22.96   \n",
              "2     2011-04-01 23:00:00       2        0           1        1  12.30   \n",
              "3     2012-09-16 09:00:00       3        0           0        1  23.78   \n",
              "4     2011-02-01 23:00:00       1        0           1        3   8.20   \n",
              "...                   ...     ...      ...         ...      ...    ...   \n",
              "8159  2012-01-14 02:00:00       1        0           0        1   6.56   \n",
              "8160  2011-12-10 09:00:00       4        0           0        1  11.48   \n",
              "8161  2011-12-18 16:00:00       4        0           0        1  11.48   \n",
              "8162  2011-02-19 07:00:00       1        0           0        1  15.58   \n",
              "8163  2012-05-02 07:00:00       2        0           1        2  22.14   \n",
              "\n",
              "       atemp  humidity  windspeed  pollution  sunlight   traffic  \n",
              "0     32.575        65    12.9980   5.354100    28.701  0.000000  \n",
              "1     26.515        52    22.0028  85.425233    22.961  0.004489  \n",
              "2     15.910        61     6.0032   2.040899    12.301  0.000242  \n",
              "3     27.275        60     8.9981  26.682772    23.781  0.004489  \n",
              "4      9.850        93    12.9980   5.851754     8.201  0.000000  \n",
              "...      ...       ...        ...        ...       ...       ...  \n",
              "8159   8.335        47    11.0014  14.953355     6.561  0.004489  \n",
              "8160  12.880        61    19.0012   7.977025    11.481  0.000000  \n",
              "8161  13.635        48    16.9979   6.916512    11.481  0.015615  \n",
              "8162  19.695        17    35.0008   0.095445    15.581  0.000242  \n",
              "8163  25.760        88    12.9980  81.965417    22.141  0.015615  \n",
              "\n",
              "[8164 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8999a40b-4cbe-43dd-a0af-d3d3a9ba43ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>season</th>\n",
              "      <th>holiday</th>\n",
              "      <th>workingday</th>\n",
              "      <th>weather</th>\n",
              "      <th>temp</th>\n",
              "      <th>atemp</th>\n",
              "      <th>humidity</th>\n",
              "      <th>windspeed</th>\n",
              "      <th>pollution</th>\n",
              "      <th>sunlight</th>\n",
              "      <th>traffic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2011-07-11 00:00:00</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>28.70</td>\n",
              "      <td>32.575</td>\n",
              "      <td>65</td>\n",
              "      <td>12.9980</td>\n",
              "      <td>5.354100</td>\n",
              "      <td>28.701</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2012-05-18 22:00:00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>22.96</td>\n",
              "      <td>26.515</td>\n",
              "      <td>52</td>\n",
              "      <td>22.0028</td>\n",
              "      <td>85.425233</td>\n",
              "      <td>22.961</td>\n",
              "      <td>0.004489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-04-01 23:00:00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12.30</td>\n",
              "      <td>15.910</td>\n",
              "      <td>61</td>\n",
              "      <td>6.0032</td>\n",
              "      <td>2.040899</td>\n",
              "      <td>12.301</td>\n",
              "      <td>0.000242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-09-16 09:00:00</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>23.78</td>\n",
              "      <td>27.275</td>\n",
              "      <td>60</td>\n",
              "      <td>8.9981</td>\n",
              "      <td>26.682772</td>\n",
              "      <td>23.781</td>\n",
              "      <td>0.004489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2011-02-01 23:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>8.20</td>\n",
              "      <td>9.850</td>\n",
              "      <td>93</td>\n",
              "      <td>12.9980</td>\n",
              "      <td>5.851754</td>\n",
              "      <td>8.201</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8159</th>\n",
              "      <td>2012-01-14 02:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6.56</td>\n",
              "      <td>8.335</td>\n",
              "      <td>47</td>\n",
              "      <td>11.0014</td>\n",
              "      <td>14.953355</td>\n",
              "      <td>6.561</td>\n",
              "      <td>0.004489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8160</th>\n",
              "      <td>2011-12-10 09:00:00</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>11.48</td>\n",
              "      <td>12.880</td>\n",
              "      <td>61</td>\n",
              "      <td>19.0012</td>\n",
              "      <td>7.977025</td>\n",
              "      <td>11.481</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8161</th>\n",
              "      <td>2011-12-18 16:00:00</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>11.48</td>\n",
              "      <td>13.635</td>\n",
              "      <td>48</td>\n",
              "      <td>16.9979</td>\n",
              "      <td>6.916512</td>\n",
              "      <td>11.481</td>\n",
              "      <td>0.015615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8162</th>\n",
              "      <td>2011-02-19 07:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15.58</td>\n",
              "      <td>19.695</td>\n",
              "      <td>17</td>\n",
              "      <td>35.0008</td>\n",
              "      <td>0.095445</td>\n",
              "      <td>15.581</td>\n",
              "      <td>0.000242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8163</th>\n",
              "      <td>2012-05-02 07:00:00</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>22.14</td>\n",
              "      <td>25.760</td>\n",
              "      <td>88</td>\n",
              "      <td>12.9980</td>\n",
              "      <td>81.965417</td>\n",
              "      <td>22.141</td>\n",
              "      <td>0.015615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8164 rows × 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8999a40b-4cbe-43dd-a0af-d3d3a9ba43ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8999a40b-4cbe-43dd-a0af-d3d3a9ba43ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8999a40b-4cbe-43dd-a0af-d3d3a9ba43ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4f22bc24-4cb9-4d94-a310-84d1461f91ea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4f22bc24-4cb9-4d94-a310-84d1461f91ea')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4f22bc24-4cb9-4d94-a310-84d1461f91ea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUdHl9-dwcQ-",
        "outputId": "63b02b73-0b9f-4986-9c35-4464ba1fcc3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8164, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8ZGfF0Ys4mv"
      },
      "source": [
        "Dropping duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f4wcqYkhwpl2"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcofd04ys6Gj"
      },
      "source": [
        "And again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi1cBt4VwtFa",
        "outputId": "723b5a15-7ee3-40f0-b895-5d796322272f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8164, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHGKF1p_O7u0"
      },
      "source": [
        "It can be seen that there were no identical samples in the data <br><br>Next, let's take a look at the types of features that exist:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmoAuEn9wynr",
        "outputId": "6ed28b6f-9d14-4ffa-812e-7c90bf978aa5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime       object\n",
              "season          int64\n",
              "holiday         int64\n",
              "workingday      int64\n",
              "weather         int64\n",
              "temp          float64\n",
              "atemp         float64\n",
              "humidity        int64\n",
              "windspeed     float64\n",
              "pollution     float64\n",
              "sunlight      float64\n",
              "traffic       float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "train_data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUVv1jCKPHHW"
      },
      "source": [
        "Based on our knowledge of the features and the data displayed above we can conclude that: <br>\n",
        "\n",
        "Our dataset consists 12 features and 8164 observations.\n",
        "The features types :\n",
        "\n",
        "<span style=\"color: orange;\">`datetime`</span> is <b><u>Datetime</u></b> data type. <br>\n",
        "\n",
        "<span style=\"color: #6699CC;\">`temp`</span>, <span style=\"color: #6699CC;\">`atemp`</span>, <span style=\"color: #6699CC;\">`humidity`</span>, <span style=\"color: #6699CC;\">`windspeed`</span>, <span style=\"color: #6699CC;\">`pollution`</span>, <span style=\"color: #6699CC;\">`sunlight`</span> and <span style=\"color: #6699CC;\">`traffic`</span> - are <b><u>Numeric</u></b> data types. <br>\n",
        "\n",
        "<span style=\"color: green;\">`holiday`</span> and <span style=\"color: green;\">`workingday`</span> - are <b><u>Boolean </b></u> data types.<br>\n",
        "\n",
        "<span style=\"color: orange;\">`season`</span> and <span style=\"color: orange;\">`weather`</span> - are <b><u>Categorical</u></b> data types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjJJIB8aRZ6y"
      },
      "source": [
        "Hence we will update the data types of each feature:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SldEaWk39Ij2"
      },
      "outputs": [],
      "source": [
        "numeric_features = [col for col in ['temp', 'atemp', 'humidity', 'exports', 'windspeed', 'pollution', 'sunlight', 'traffic'] if col in train_data.columns]\n",
        "boolean_features = [col for col in ['holiday', 'workingday'] if col in train_data.columns]\n",
        "categorical_features = [col for col in ['season', 'weather'] if col in train_data.columns]\n",
        "datetime_features = ['datetime']\n",
        "# features = TODO put all inside maybe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P8T3mJ4ARg-C"
      },
      "outputs": [],
      "source": [
        "def change_data_types(df):\n",
        "\n",
        "    numeric_features = [col for col in ['temp', 'atemp', 'humidity', 'exports', 'windspeed', 'pollution', 'sunlight', 'traffice'] if col in df.columns]\n",
        "    boolean_features = [col for col in ['holiday', 'workingday'] if col in df.columns]\n",
        "    categorical_features = [col for col in ['season', 'weather'] if col in df.columns]\n",
        "\n",
        "    # Convert 'datetime' to datetime type\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "    # Change numeric features to numeric data type\n",
        "    df[numeric_features] = df[numeric_features].astype(float)\n",
        "\n",
        "    # Change boolean features to boolean data type\n",
        "    for col in boolean_features:\n",
        "        df[col] = df[col].where(df[col].notnull(), np.nan).astype('boolean')\n",
        "\n",
        "    # Change categorical features to categorical data type\n",
        "    df[categorical_features] = df[categorical_features].astype('category')\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wCejpHL4R6_E"
      },
      "outputs": [],
      "source": [
        "train_data = change_data_types(train_data)\n",
        "train_with_labels = change_data_types(train_with_labels)\n",
        "test_data = change_data_types(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeaqjzpXTfZO"
      },
      "source": [
        "And after the changes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhTBhuWmTgM1",
        "outputId": "e7318bae-67ce-4988-ee62-f4296e515449"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime      datetime64[ns]\n",
              "season              category\n",
              "holiday              boolean\n",
              "workingday           boolean\n",
              "weather             category\n",
              "temp                 float64\n",
              "atemp                float64\n",
              "humidity             float64\n",
              "windspeed            float64\n",
              "pollution            float64\n",
              "sunlight             float64\n",
              "traffic              float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvrLfHgCvD_9",
        "outputId": "f1353b5d-ce03-4be2-a0d1-e3c61afb7b07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime      datetime64[ns]\n",
              "season              category\n",
              "holiday              boolean\n",
              "workingday           boolean\n",
              "weather             category\n",
              "temp                 float64\n",
              "atemp                float64\n",
              "humidity             float64\n",
              "windspeed            float64\n",
              "pollution            float64\n",
              "sunlight             float64\n",
              "traffic              float64\n",
              "count                  int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_with_labels.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W55ei0QSyPJ7"
      },
      "source": [
        " ## **Date time Features - Time Series**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWtBedZrt8Ca"
      },
      "source": [
        "In this section we will check the time series attributes that the data has:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From looking at the datetime feature we see that we can seperate it to sub features: Day, Month, Year and Hour"
      ],
      "metadata": {
        "id": "1eqiR7fz8kEE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW1SZAyppgEW"
      },
      "source": [
        "But before separating the 'datetime' feature to sub features, we will check whether there are minutes and hours:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9pXjo0SpP8s",
        "outputId": "18b10e58-fae5-44a9-d9a5-5159964d501d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All unique times in train_data['datetime'], sorted:\n",
            "00:00:00\n",
            "01:00:00\n",
            "02:00:00\n",
            "03:00:00\n",
            "04:00:00\n",
            "05:00:00\n",
            "06:00:00\n",
            "07:00:00\n",
            "08:00:00\n",
            "09:00:00\n",
            "10:00:00\n",
            "11:00:00\n",
            "12:00:00\n",
            "13:00:00\n",
            "14:00:00\n",
            "15:00:00\n",
            "16:00:00\n",
            "17:00:00\n",
            "18:00:00\n",
            "19:00:00\n",
            "20:00:00\n",
            "21:00:00\n",
            "22:00:00\n",
            "23:00:00\n"
          ]
        }
      ],
      "source": [
        "unique_times = train_data['datetime'].dt.strftime('%H:%M:%S').unique()\n",
        "\n",
        "sorted_times = sorted(unique_times)\n",
        "\n",
        "print(\"All unique times in train_data['datetime'], sorted:\")\n",
        "for time in sorted_times:\n",
        "  print(time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KAN2nYqQFqu",
        "outputId": "4485e086-5a41-45a8-b0bf-8dc52770b327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique hours: 24\n"
          ]
        }
      ],
      "source": [
        "# Count the number of unique hours\n",
        "num_unique_hours = len(unique_times)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Number of unique hours: {num_unique_hours}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qALNNuLJpiS0"
      },
      "source": [
        "We observe that the 'datetime' column only includes hours without minutes and seconds. Therefore, we will add new features specifically for hours, days, month and year.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4lTk5XzTN1Y"
      },
      "source": [
        "As you can see, bicycles can be rented at any time of the day."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO - NEED TO DEL!!!"
      ],
      "metadata": {
        "id": "KMYzLi0-tBVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yD-IxI5Ix8mg"
      },
      "outputs": [],
      "source": [
        "# def timeseries_features(df):\n",
        "\n",
        "#   df['day'] = df['datetime'].dt.day\n",
        "#   df['month'] = df['datetime'].dt.month\n",
        "#   df['year'] = df['datetime'].dt.year\n",
        "#   df['hour'] = df['datetime'].dt.hour\n",
        "\n",
        "#   return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def timeseries_features(df):\n",
        "\n",
        "  df['dayInWeek'] = df['datetime'].dt.dayofweek\n",
        "  df['dayInMonth'] = df['datetime'].dt.day\n",
        "  df['month'] = df['datetime'].dt.month\n",
        "  df['year'] = df['datetime'].dt.year\n",
        "  df['hour'] = df['datetime'].dt.hour\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "zkrKHB-CtGNv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def timeseries_category(df):\n",
        "\n",
        "  # Change them to category type data\n",
        "  df['dayInWeek'] = df['dayInWeek'].astype('category')\n",
        "  df['dayInMonth'] = df['dayInMonth'].astype('category')\n",
        "  df['month'] = df['month'].astype('category')\n",
        "  df['year'] = df['year'].astype('category')\n",
        "  df['hour'] = df['hour'].astype('category')\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "FCArRLpLVBpH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def timeseries_engineering(df):\n",
        "\n",
        "  df = timeseries_features(df)\n",
        "  df = timeseries_category(df)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Uj_dYCd5Vavl"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO - NEED TO DEL!!!"
      ],
      "metadata": {
        "id": "xspqKn_wsxWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # TODO Added : change accordingly\n",
        "# def timeseries_features_v2(df):\n",
        "\n",
        "#   df['dayInWeek'] = df['datetime'].dt.dayofweek\n",
        "#   df['dayInMonth'] = df['datetime'].dt.day\n",
        "#   df['month'] = df['datetime'].dt.month\n",
        "#   df['year'] = df['datetime'].dt.year\n",
        "#   df['hour'] = df['datetime'].dt.hour\n",
        "\n",
        "#   return df"
      ],
      "metadata": {
        "id": "ixfv_G0yTPoT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjlRlec1prGr"
      },
      "source": [
        "These new features will replace the datetime feature when training the models. For now, we will leave the datetime feature in order to perform additional analysis on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IcN5Kvu21Vg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "c442d42b-8fa2-48d6-9bda-7a47506d2f0b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'day'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'day'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0c4be31b2003>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeseries_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_with_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeseries_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_with_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeseries_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-b00e1efad3b0>\u001b[0m in \u001b[0;36mtimeseries_engineering\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeseries_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeseries_category\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-b646901e92e9>\u001b[0m in \u001b[0;36mtimeseries_category\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# Change them to category type data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'day'"
          ]
        }
      ],
      "source": [
        "train_data = timeseries_engineering(train_data)\n",
        "train_with_labels = timeseries_engineering(train_with_labels)\n",
        "test_data = timeseries_engineering(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLLHfyDVqBXF"
      },
      "outputs": [],
      "source": [
        "train_with_labels.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UgHZc3jqDPW"
      },
      "source": [
        "Let's look at the sorted data and maybe draw some new conclusions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvD66sQfqEc8"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = str(train_data['datetime'].min())\n",
        "end = str(train_data['datetime'].max())\n",
        "\n",
        "print(\"Start: \" + start[:10], \"   Time: \" + start[11:], \"\\nEnd:   \" + end[:10], \"   Time: \" + end[11:])"
      ],
      "metadata": {
        "id": "kKFwsf_dCMfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data is all from the beginning of 2011 until December 19, 2012."
      ],
      "metadata": {
        "id": "0xEO6FZECPq4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kydw-uJSqIma"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_with_labels['datetime'], train_with_labels['count'])\n",
        "plt.title('Time Series Plot')\n",
        "plt.xlabel('Datetime')\n",
        "plt.ylabel('Bike Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ketvH-k-WBdu"
      },
      "source": [
        "It can be seen that in the first half of 2011 there was a monthly increase in the amount of rented bicycles, which changed direction to decrease in the second half of 2011. The amount of rented bicycles increased again starting from 2012 until the last quarter of that year.<br>\n",
        "Moreover, we can see in the diagram many intervals of identical sizes. Let's try to understand better what they say:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_actual_and_missing_dates(df):\n",
        "  unique_dates = df['datetime'].dt.date.unique()\n",
        "  actual_date_strings = [str(date) for date in unique_dates]\n",
        "\n",
        "  # Convert 'datetime' column to DatetimeIndex\n",
        "  df_datetime_index = pd.DatetimeIndex(df['datetime'])\n",
        "\n",
        "  # Get unique dates in the same format as 'expected_dates'\n",
        "  expected_dates = pd.date_range(start=df_datetime_index.min(), end=df_datetime_index.max())\n",
        "\n",
        "  # Convert expected_dates to an array of datetime.date objects\n",
        "  expected_dates_array = np.array(expected_dates.date)\n",
        "\n",
        "  expected_dates_strings = [str(date) for date in expected_dates_array]\n",
        "\n",
        "  # Convert the date strings to sets\n",
        "  expected_dates_set = set(expected_dates_strings)\n",
        "  actual_dates_set = set(actual_date_strings)\n",
        "\n",
        "  # Find the dates in expected_dates_set but not in actual_dates_set\n",
        "  missing_dates = expected_dates_set - actual_dates_set\n",
        "\n",
        "  # Convert the result back to a list\n",
        "  missing_dates_list = list(missing_dates)\n",
        "  actual_dates_list = list(actual_dates_set)\n",
        "\n",
        "  # Sort the list to have the dates in ascending order\n",
        "  missing_dates = sorted(missing_dates_list)\n",
        "  actual_dates = sorted(actual_dates_list)\n",
        "\n",
        "  print(f'Expected Days: {len(expected_dates_set)}')\n",
        "  print(f'Actual Days: {len(actual_date_strings)}')\n",
        "  print(f'Missing Days: {len(missing_dates)}')\n",
        "\n",
        "  return actual_dates, missing_dates"
      ],
      "metadata": {
        "id": "4vOonNZM5dkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_dates_train, missing_dates_train = get_actual_and_missing_dates(train_data)"
      ],
      "metadata": {
        "id": "nGc9Z-f-7lHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_dates_test, missing_dates_test = get_actual_and_missing_dates(test_data)"
      ],
      "metadata": {
        "id": "sFvSzNwT8YeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 263 days in two years when bicycles were not rented."
      ],
      "metadata": {
        "id": "SXYuJRMlCg0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the missing dates in some ways:"
      ],
      "metadata": {
        "id": "7zu4rNBS506u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_missing_dates(missing_dates):\n",
        "  if len(missing_dates) > 0:\n",
        "      print(\"Missing dates:\")\n",
        "      # missing_dates = sorted(missing_dates)\n",
        "\n",
        "      month_before = '01'\n",
        "      for date in missing_dates:\n",
        "        month = str(date)[5:8]\n",
        "        if month != month_before:\n",
        "          print(\"-----------------------------\")\n",
        "        print(str(date)[:10])\n",
        "        month_before = month\n",
        "  else:\n",
        "      print(\"No missing dates\")"
      ],
      "metadata": {
        "id": "7jFEW7Qb5fX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_missing_dates(missing_dates_train)"
      ],
      "metadata": {
        "id": "9T5l6nHZ8PUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_missing_dates(missing_dates_test)"
      ],
      "metadata": {
        "id": "DucTwdGO8hp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that bicycles were not rented on the days starting from the 20th of the month until the end of the month.<br>\n",
        "Next, another look:"
      ],
      "metadata": {
        "id": "GkHvpclvC-zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert date strings to datetime objects\n",
        "def convert_to_datetime(date_str):\n",
        "    return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
        "\n",
        "def plot_actual_dates(missing_dates):\n",
        "  # Define the date range from 1/1/2011 to 1/1/2013\n",
        "  start_date = datetime(2011, 1, 1)\n",
        "  end_date = datetime(2013, 1, 1)\n",
        "  date_range = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
        "\n",
        "  # Convert missing_dates to datetime objects\n",
        "  missing_dates_datetime = [convert_to_datetime(date_str) for date_str in missing_dates]\n",
        "\n",
        "  # Create a list of binary values indicating if each date is missing or not\n",
        "  missing_flags = [1 if date in missing_dates_datetime else 0 for date in date_range]\n",
        "\n",
        "  # Identify consecutive ranges of missing and not missing dates\n",
        "  ranges = []\n",
        "  current_range = []\n",
        "  for i, flag in enumerate(missing_flags):\n",
        "      if flag == 1:\n",
        "          if current_range:\n",
        "              ranges.append(current_range)\n",
        "              current_range = []\n",
        "      else:\n",
        "          current_range.append(date_range[i])\n",
        "\n",
        "  # Plotting\n",
        "  plt.figure(figsize=(15, 3))  # Adjust the figure size here\n",
        "  for date_range_subset in ranges:\n",
        "      plt.plot(date_range_subset, [1] * len(date_range_subset), color='blue')\n",
        "\n",
        "  plt.xlabel('Date')\n",
        "  plt.ylabel('Missing Dates (1)')\n",
        "  plt.title('Missing Dates Over Time')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "2vSniuq55hms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_actual_dates(missing_dates_train)"
      ],
      "metadata": {
        "id": "d6Ku_fst8_qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_actual_dates(missing_dates_test)"
      ],
      "metadata": {
        "id": "v4C_bSwR9KRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data we have covers only the first 20 days of each month from January 2011 to November 2012. We don't have information for the other days of each month. Both the Train and Test datasets follow this pattern.\n",
        "\n",
        "Because of this, using typical time series models or analyses might not be suitable. <br>The Test set shares the same timeframe but includes different hours within each day, keeping only the specific days constant.\n"
      ],
      "metadata": {
        "id": "UKasXl7k9M7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights and Comments:**\n",
        "\n",
        "- <u>Datetime:</u>\n",
        "\n",
        "    Each timestamp in the 'datetime' column is unique, signifying that there are no duplicated timestamp values. We have separated this column to handle the information individually, extracting details such as:\n",
        "\n",
        "    * day\n",
        "    * month\n",
        "    * year\n",
        "    * hour"
      ],
      "metadata": {
        "id": "K58u3PSVxJv7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMEpr4k3j2G6"
      },
      "source": [
        "## **Features Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwF74W59qwf_"
      },
      "source": [
        "### Categorical Features distribution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_categorical_frequency(df, categorical_features):\n",
        "    num_features = len(categorical_features)\n",
        "\n",
        "    plt.figure(figsize=(14, 12))\n",
        "\n",
        "    for i, feature in enumerate(categorical_features, 1):\n",
        "        plt.subplot(2, 2, i)\n",
        "        sns.countplot(x=feature, data=df)\n",
        "        plt.ylabel(f'Frequency of {feature.capitalize()}', fontsize=14)\n",
        "        plt.xlabel(f'{feature.capitalize()}', fontsize=12)\n",
        "        plt.xticks(fontsize=10)\n",
        "        plt.yticks(fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "categorical_features_list = ['season', 'holiday', 'workingday', 'weather']\n",
        "plot_categorical_frequency(train_data, categorical_features_list)\n"
      ],
      "metadata": {
        "id": "z9qfzYaqAdoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Qd14_Io8Eb"
      },
      "source": [
        "**From a first look, we can see that:**\n",
        "Each count plot has a y-axis label indicating the frequency of the respective categorical feature.\n",
        "* The 'season' feature has a similar number of instances in the data for every season.\n",
        "* The majority of instances in the data have clear weather.\n",
        "* Most of the days in the data are not holidays and possibly workdays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLMKiIE07p4i"
      },
      "source": [
        "Let's see the numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ1NDim47tRO"
      },
      "outputs": [],
      "source": [
        "def categorial_repr_of_features(df, features):\n",
        "  for column in features:\n",
        "      categories = df[column].value_counts()\n",
        "      print(f\"Categories in column '{column}':\")\n",
        "      display(categories)\n",
        "      # Noting when the categories are unique.\n",
        "      if len(categories) == df.shape[0]:\n",
        "          print(\"Each category is different.\")\n",
        "      print(f\"Number of categories: \" , len(categories), '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CV_pUvh8L04"
      },
      "outputs": [],
      "source": [
        "categorial_repr_of_features(train_data, ['holiday', 'workingday', 'season', 'weather'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJZVAe-_8cfl"
      },
      "source": [
        "As we expected, there is an only 1 sample of when the weather is 4.<br>\n",
        "In addition, for our research it is reasonable to estimate that in this weather the amount of bicycle rentals should decrease significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights and Comments:**\n",
        "\n",
        "- <u>Season:</u>\n",
        "\n",
        "    There are four distinct categories in the 'season' column (1, 2, 3, 4) with varying counts.\n",
        "    The distribution suggests that each season is well-represented in the dataset.\n",
        "\n",
        "\n",
        "- <u>Holiday and Workingday:</u>\n",
        "\n",
        "    The 'holiday' column has two categories (0, 1) with the majority being non-holiday days (0).\n",
        "    The 'workingday' column has two categories (0, 1), with a higher count for working days (1).\n",
        "\n",
        "\n",
        "- <u>Weather:</u>\n",
        "\n",
        "    The 'weather' column has four categories (1, 2, 3, 4), with the majority falling under category 1.\n",
        "    Category 4 appears to have only one occurrence and might be an outlier or error."
      ],
      "metadata": {
        "id": "5Lpzhac8wpq4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBRxfPjgrXVc"
      },
      "source": [
        "###  **Numerical Features Distribution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmGmZ9pmTp4N"
      },
      "source": [
        "Next, we want to observe some statistics to understand what data we have: <br>\n",
        "(Note that we are only looking at the numerical features and boolean features - which are represented as numbers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMRLF5pcxVqV"
      },
      "outputs": [],
      "source": [
        "# Get the summary statistics of the features\n",
        "print(\"\\nSummary statistics of the features:\")\n",
        "train_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOUpoRcnUZY3"
      },
      "source": [
        "Based on this information, we can make the following observations: (Generated by GPT)\n",
        "\n",
        "Here are some conclusions and observations that can be drawn from the train_data dataframe based on the provided statistics:\n",
        "\n",
        "- <u>Temperature and Apparent Temperature (temp & atemp):</u>\n",
        "\n",
        "    The mean temperature is around 20.24°C, with a standard deviation of 7.80°C.\n",
        "    Apparent temperature (atemp) has a similar distribution to temperature.\n",
        "\n",
        "\n",
        "- <u>Humidity (in %):</u>\n",
        "\n",
        "    The average humidity is approximately 61.84%, with a standard deviation of 19.26%.\n",
        "    The minimum humidity is 0%, which might be an outlier or missing data.\n",
        "\n",
        "\n",
        "- <u>Windspeed:</u>\n",
        "\n",
        "    The average windspeed is 12.79, with a standard deviation of 8.21.\n",
        "    There is a wide range of windspeed values, with a minimum of 0 and a maximum of 56.9979.\n",
        "\n",
        "\n",
        "- <u>Pollution:</u>\n",
        "\n",
        "    The pollution level has a mean of 47.15, but with a high standard deviation of 72.88.\n",
        "    The pollution values range from a minimum of 0.000304 to a maximum of 754.30, suggesting potential outliers.\n",
        "\n",
        "\n",
        "- <u>Sunlight:</u>\n",
        "\n",
        "    The average sunlight is 20.25, with a standard deviation of 7.80.\n",
        "    Sunlight values range from a minimum of 0.821 to a maximum of 41.001.\n",
        "    Seems like it is correlated to temperature...\n",
        "\n",
        "\n",
        "- <u>Traffic:</u>\n",
        "\n",
        "    The traffic variable has a very low mean of 0.00499, with a standard deviation of 0.00632.\n",
        "    The majority of the values seem to be close to zero, suggesting sparse traffic data. Its impact on the bicycle rental demand needs exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR3CuSyt5GAw"
      },
      "source": [
        "Now we would like to understand how the features are structured. <br>\n",
        "Are the values in each feature repeated ...? What are common values in every feature ...? Is an attribute a representative attribute with different values ...?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_ls8lGY4w_2"
      },
      "outputs": [],
      "source": [
        "categorial_repr_of_features(train_data, numeric_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3or4IhI5f4-"
      },
      "source": [
        "**Insights and Comments:**\n",
        "\n",
        "- <u>Temperature and Apparent Temperature (temp, atemp):</u>\n",
        "\n",
        "    Both 'temp' and 'atemp' columns have a wide range of values, with multiple occurrences for each temperature.\n",
        "    These features appear to have been discretized or rounded, resulting in multiple instances of the same temperature.\n",
        "\n",
        "\n",
        "- <u>Humidity:</u>\n",
        "\n",
        "    The 'humidity' column has 87 unique values, indicating a diverse range of humidity levels in the dataset.\n",
        "\n",
        "\n",
        "- <u>Windspeed:</u>\n",
        "\n",
        "    The 'windspeed' column has 29 unique values, with a dominant occurrence of 0.0000.\n",
        "    It's possible that the windspeed values have been discretized, and 0.0000 might represent calm or very low windspeed.\n",
        "\n",
        "\n",
        "- <u>Pollution:</u>\n",
        "\n",
        "    Each value in the 'pollution' column is unique, indicating a diverse range of pollution levels.\n",
        "    This feature appears to be continuous and might require further investigation for outliers.\n",
        "\n",
        "\n",
        "- <u>Sunlight:</u>\n",
        "\n",
        "    Similar to 'temp' and 'atemp', the 'sunlight' column has a variety of values with multiple occurrences for each sunlight level.\n",
        "\n",
        "\n",
        "- <u>Traffic:</u>\n",
        "\n",
        "    The 'traffic' column has four distinct values, with the majority being either 0.000000 or 0.000242.\n",
        "    This variable might represent traffic intensity, and the low values suggest sparse traffic data. <br>\n",
        "    Moreover, it behaves like a categorical variable with only 4 possible values - STRANGE. <br>\n",
        "    Also, the numbers are very low and very close to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5d25OzYe2Jt"
      },
      "source": [
        "Let's take a look on the distribution of the numerical features to draw some conclusions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNXUAbeCe3ju"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.rc('axes', labelsize=4)  # Adjust label font size\n",
        "plt.rc('xtick', labelsize=8)  # Adjust x-axis tick font size\n",
        "plt.rc('ytick', labelsize=8)  # Adjust y-axis tick font size\n",
        "plt.rc('legend', fontsize=8)  # Adjust legend font size\n",
        "\n",
        "train_data_subset = train_data.drop(columns=['datetime'])\n",
        "\n",
        "# Plot histograms\n",
        "train_data_subset.hist(ax=plt.gca())\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ-nSLVyiC7N"
      },
      "source": [
        "* Seems that there might be some outliers in \"windspeed\" and \"pollution\" as stated above.\n",
        "\n",
        "**Another look:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEOgONRsh9x6"
      },
      "outputs": [],
      "source": [
        "def create_distribution_graph(df):\n",
        "    numeric_cols = df.select_dtypes(include=[float, int]).columns\n",
        "    num_cols = len(numeric_cols)\n",
        "    rows = int(math.sqrt(num_cols))\n",
        "    cols = int(math.ceil(num_cols / rows))\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(35, 20))\n",
        "\n",
        "    plot_index = 0\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            if plot_index < num_cols:\n",
        "                col = numeric_cols[plot_index]\n",
        "                df[col].plot.density(ax=axes[i, j])\n",
        "                axes[i, j].set_title(col.capitalize())\n",
        "                plot_index += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "create_distribution_graph(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcVfxbr4jGSO"
      },
      "source": [
        "* \"Humidity\" and \"Pollution\" show some type of normality, maybe after some tweaking a better conclusion could be drawn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2yJ-B81juCN"
      },
      "source": [
        "We will deal with outliers later.<br>\n",
        "Hence, let's apply log transformation on these features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vpbdphwkA6G"
      },
      "outputs": [],
      "source": [
        "# Function that creates a distribution graph for all the numeric features\n",
        "def create_distribution_graph(df):\n",
        "    for i, col in enumerate(df.select_dtypes(include=[float, int])):\n",
        "        transformed_data = np.log1p(df[col])  # Apply logarithm transformation\n",
        "        transformed_data.plot.density()\n",
        "        plt.title(col.capitalize() + \" (Log Transformed)\")\n",
        "        plt.show()\n",
        "\n",
        "create_distribution_graph(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It can be seen that some features appear to follow a normal distribution (or something close enough) like: humidity and log of temp and log of sunlight.\n",
        "* the distrdistribution of temp and sunlight look the same."
      ],
      "metadata": {
        "id": "efhOjA2m2jn6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCeLkOkokIWH"
      },
      "source": [
        "## **Label Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHBcca3OmCaT"
      },
      "outputs": [],
      "source": [
        "train_with_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YLEkalylQAT"
      },
      "outputs": [],
      "source": [
        "plt.hist(train_with_labels['count'], bins=30, edgecolor='black')\n",
        "plt.title('Distribution of Rental Count')\n",
        "plt.xlabel('Rental Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PjYff6xyWDw"
      },
      "source": [
        "In this chart we can see how many times there was any amount of bike rentals. For example, there were more than 1750 times (hours) where there were 0 bike rentals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "875HWwVdtJcs"
      },
      "source": [
        "**Box Plot for Target Variable vs. Categorical and Boolean Features to explore how the target variable varies across different categories:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-EiF7z5tSGZ"
      },
      "source": [
        "Let's see some information about the count of the bike rentals there were in each category:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Box Plot for Target Variable vs. Categorical and Boolean Features to explore how the target variable varies across different categories.\n",
        "categorical_features = ['season', 'holiday', 'workingday', 'weather']\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(categorical_features), figsize=(16, 6))\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    sns.boxplot(x=feature, y='count', data=train_with_labels, ax=axes[i])\n",
        "    axes[i].set_title(f'Boxplot of Count by {feature}')\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DY3ywvF2o0DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can understand things about the median and the percentiles of the amount of bike rentals in each category."
      ],
      "metadata": {
        "id": "bQkPeQ7HzytX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze the distribution of bike rentals across distinct categories within specific categorical features ('season,' 'holiday,' 'workingday,' and 'weather')."
      ],
      "metadata": {
        "id": "ayXFDEcUFHDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total count of bike rentals\n",
        "total_rentals = train_with_labels['count'].sum()\n",
        "\n",
        "# List of categorical features\n",
        "categorical_features = ['season', 'holiday', 'workingday', 'weather']\n",
        "\n",
        "# Create subplots with a single row and multiple columns\n",
        "fig, axes = plt.subplots(nrows=1, ncols=len(categorical_features), figsize=(15, 5))\n",
        "\n",
        "# Iterate over each categorical feature\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    # Calculate the percentage of bike rentals for each category\n",
        "    percentages = (train_with_labels.groupby(feature)['count'].sum() / total_rentals) * 100\n",
        "\n",
        "    # Create a bar plot for each categorical feature\n",
        "    sns.barplot(x=percentages.index, y=percentages, ax=axes[i])\n",
        "    axes[i].set_title(f'Percentage of Bike Rentals by {feature.capitalize()}')\n",
        "    axes[i].set_ylabel('Percentage')\n",
        "\n",
        "# Adjust the layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JfsnxB7WEzM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "These charts calculate the percentage of bike rentals for each category. <br>For example, only 15 percent of the bike rentals happened in season 1.<br>\n",
        "An overwhelming majority of the bike rentals happened on weekdays and not on holidays.<br>\n",
        "\n",
        "It is difficult to draw conclusions from these data due to the imbalance in the data. We will have difficulty understanding the behavior of the bicycle rentals in each of the categories due to the imbalance in the data and we would like to normalize this data in order to better study the nature of the bicycle rentals. For example, there are many more weekdays than holidays - the data will be skewed so that naturally more bikes will be rented on weekdays because there are more weekdays in the data.\n"
      ],
      "metadata": {
        "id": "28zqFnz409qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_bike_rentals = train_with_labels['count'].sum()\n",
        "\n",
        "print(f\"Total number of bike rentals in the dataset: {total_bike_rentals}\")"
      ],
      "metadata": {
        "id": "X-sOBa1gRYEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of categorical features\n",
        "categorical_features = ['season', 'weather', 'holiday', 'workingday']\n",
        "\n",
        "# Loop through each categorical feature\n",
        "for feature in categorical_features:\n",
        "    # Calculate the sum of rental bikes for each category\n",
        "    total_count_by_category = train_with_labels.groupby(feature)['count'].sum()\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Total rental bikes by {feature}:\")\n",
        "    print(total_count_by_category)\n",
        "    print(\"\\n\" + \"-\"*30 + \"\\n\")"
      ],
      "metadata": {
        "id": "kQoTI7b-KZEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the imbalance in the data and provide a fair comparison across categories, we toke the sum of bike rentals and divided it by the amount of each category instance. then we got the average count of rentals per unit in every categorial feature. This way, we normalized the rental counts by the frequency of each category.\n"
      ],
      "metadata": {
        "id": "0cp5y4CFStuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates the mean number of rental bikes for each category within the specified categorical features:"
      ],
      "metadata": {
        "id": "Zg_yveCXOHAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of categorical features\n",
        "categorical_features = ['season', 'weather', 'holiday', 'workingday', 'day']\n",
        "\n",
        "# Loop through each categorical feature\n",
        "for feature in categorical_features:\n",
        "    # Calculate the mean number of rental bikes for each category\n",
        "    mean_count_by_category = train_with_labels.groupby(feature)['count'].mean()\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Mean rental bikes by {feature}:\")\n",
        "    print(mean_count_by_category)\n",
        "    print(\"\\n\" + \"-\"*30 + \"\\n\")"
      ],
      "metadata": {
        "id": "ZCGJBUJFJvqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a set of pie charts to visualize the mean number of rental bikes for each category within the specified categorical features:"
      ],
      "metadata": {
        "id": "fFqZOfS1Onao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of categorical features\n",
        "categorical_features = ['season', 'weather', 'holiday', 'workingday', 'day']\n",
        "\n",
        "# Set up subplots\n",
        "fig, axes = plt.subplots(1, len(categorical_features), figsize=(20, 6))\n",
        "\n",
        "# Loop through each categorical feature\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    # Calculate the mean number of rental bikes for each category\n",
        "    mean_count_by_category = train_with_labels.groupby(feature)['count'].mean()\n",
        "\n",
        "    # Plot the means in a pie chart\n",
        "    axes[i].pie(mean_count_by_category, labels=mean_count_by_category.index, autopct='%1.1f%%', startangle=90)\n",
        "    axes[i].set_title(f\"Mean Rental Bikes by {feature}\")\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mlVk6Zu7LwIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From the visualization above, we get the following insights:**\n",
        "\n",
        "* **Season:**\n",
        "Rentals tend to be lower in Season 1 (Spring) compared to the other seasons.\n",
        "Season 3 (Fall) has the highest mean rental count.\n",
        "\n",
        "* **Weather:**\n",
        "Weather conditions 1 (Clear, Few clouds) have the highest mean rental count.\n",
        "Weather condition 3 (Light Snow, Light Rain, Thunderstorm) has the lowest mean rental count.\n",
        "\n",
        "\n",
        "* **Holiday:**\n",
        "There is a slight decrease in mean rental count on holidays compared to non-holidays, but the difference is not very significant.\n",
        "\n",
        "* **Workingday:**\n",
        "Mean rental counts are slightly higher on working days compared to non-working days.<br><br>\n",
        "\n",
        "\n",
        " The insights offer an overview of average rental counts across various categorical features. It seems that factors such as season and weather exert more noticeable effects on rental counts compared to holiday or working day status. Overall, although there are some variations in mean rental counts across different feature categories, **the differences may not be highly significant.**"
      ],
      "metadata": {
        "id": "Flmq2SLel-8V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SdCPhcR7i6V"
      },
      "outputs": [],
      "source": [
        "#This code calculates and plots the percentage of bike rentals by day of the week.\n",
        "\n",
        "total_counts_by_day = train_with_labels.groupby(train_with_labels['datetime'].dt.dayofweek)['count'].sum()\n",
        "percentage_rentals_by_day = (total_counts_by_day / total_counts_by_day.sum()) * 100\n",
        "\n",
        "days_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(percentage_rentals_by_day, labels=days_labels, autopct='%1.1f%%', startangle=90, colors=plt.cm.Set3.colors)\n",
        "plt.title('Percentage of Bike Rentals by Day of the Week')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsYiqWpf28Xk"
      },
      "source": [
        "From the visualization above of the bike rental percentages for each day, we observe a consistent pattern throughout the week. The rental percentage remains relatively stable at between 14.1% and 14.9% for each day, except for Sunday, where it slightly decreases to 13.4%. This slight deviation aligns with expectations as Sunday is typically a non-working day. But we consider the variation as minimal, so we could say it is a fairly uniform distribution of bike rentals across the days of the week, with Sunday being only marginally lower but still in close proximity to the 14% mark observed on other days."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation"
      ],
      "metadata": {
        "id": "-OtGfcjyFnSI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghLfcd12nO1z"
      },
      "source": [
        "**trying to learn about the relationships between numerical features and the target variable using a correlation matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Vfif2JEU57"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "correlation_matrix = train_with_labels.corr()\n",
        "correlation_with_label = correlation_matrix['count']\n",
        "\n",
        "# Remove the label feature from the correlation\n",
        "correlation_with_label = correlation_with_label.drop('count')\n",
        "\n",
        "correlation_with_label.plot(kind='bar', color='skyblue')\n",
        "plt.title('Correlation with Count Label')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Correlation')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uka-ZzahEgWT"
      },
      "source": [
        "**Insights from the presentation of the correlation:**\n",
        "\n",
        "*  The correlation tables provide insights into the relationships between variables in a dataset, especially focusing on how variables are associated with each other and, in the case of the second table, their correlation with a specific target variable ('count').\n",
        "*   'polution' feature is highly correlated with the label (0.6), that is, it can be concluded that as air pollution increases, there are more bike rentals and vice versa.\n",
        "*   'traffic' has a very low correlation with both the target variable ('count') and other features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYVF0x3IlRz3"
      },
      "source": [
        "Let's make a full correlation matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkRYCIPjlShC"
      },
      "outputs": [],
      "source": [
        "def corr_matrix(df):\n",
        "  corr_matrix = df.corr().round(1)  # Round the correlation values to 1 decimal place\n",
        "  plt.figure(figsize=(12, 10))\n",
        "  sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=.5)  # Set fmt='.1f' to display 1 decimal place\n",
        "  plt.title('Correlation Heatmap')\n",
        "  plt.show()\n",
        "\n",
        "corr_matrix(train_with_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNbhaS30lp52"
      },
      "source": [
        "This function creates a visual representation of the correlation matrix using a color-coded heatmap.<br>\n",
        "It displays a grid of squares, where each square represents the correlation between two features.\n",
        "\n",
        "Here are some general observations we can make from the heatmap:\n",
        "\n",
        "<b> Strong positive correlation:</b> If two features have a high positive correlation, it suggests that as one feature increases, the other feature tends to increase as well. Conversely, if two features have a strong negative correlation (close to -1), it means that as one feature increases, the other feature tends to decrease.\n",
        "\n",
        "<b>Weak or no correlation:</b> If the correlation coefficient is close to 0, it indicates a weak or no linear relationship between the features. This means that changes in one feature do not necessarily correspond to changes in the other feature.\n",
        "\n",
        "<b>Redundant or highly correlated features:</b> High correlation values between pairs of features might indicate that these features provide similar information, in our case: sunlight & temp & atemp. Sunlight, temperature, and average temperature (atemp) show multicollinearity because they're closely related in weather patterns. Clear and sunny days typically come with both higher temperatures, making these variables correlated and potentially redundant in predicting outcomes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights from the correlation matrix:**\n",
        "\n",
        "* As we saw before, the label \"count\" is highly correlated with the feature \"pollution\" (0.6).\n",
        "* Correlation of 0.4 (not too low) with: temp, atemp, sunlight with count.\n",
        "* We will observe that there are also numerous instances of zeros, indicating an absolute absence of correlation.\n",
        "* We can see that there is high correlation (1) between the features \"temp\", \"atemp\" and \"sunlight\"."
      ],
      "metadata": {
        "id": "mPE4rSuRITQE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "456sdY9dkiI_"
      },
      "source": [
        "**VIF:**\n",
        "\n",
        "Let's take a look in general. <br>\n",
        "In this part we will use a new method which was not learnt in class- the VIF method.\n",
        "We want primarily observe whether a feature has a high correlation with other features based on its Variance Inflation Factor (VIF) value.<br>\n",
        "A high VIF value indicates multicollinearity, which suggests a strong correlation between the feature and other features in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtmIghpJkj8X"
      },
      "outputs": [],
      "source": [
        "# Select numerical features (excluding the categorical features)\n",
        "numerical_features = train_data.select_dtypes(include=['float64', 'bool'])\n",
        "\n",
        "# Convert boolean features to numeric (0 and 1)\n",
        "boolean_features = numerical_features.select_dtypes(include='bool')\n",
        "boolean_features = numerical_features.dropna()\n",
        "numerical_features[boolean_features.columns] = boolean_features.astype(int)\n",
        "\n",
        "# Remove any rows with null values in the selected features\n",
        "numerical_features = numerical_features.dropna()\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Variable\"] = numerical_features.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(numerical_features.values.astype(float), i) for i in range(numerical_features.shape[1])]\n",
        "\n",
        "print(\"VIF for all features except category features:\")\n",
        "print(vif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsWYTkUZk1ky"
      },
      "source": [
        "To calculate the VIF, we regress each predictor variable against all the other predictor variables in the model. The VIF for each variable is then computed as the ratio of the variance of the estimated regression coefficient to the variance of the coefficient if that variable was uncorrelated with the other predictors.\n",
        "\n",
        "Features with VIF values close to 1 (around or below 1) indicate low multicollinearity. These features are relatively independent of each other when predicting the target variable.<br>\n",
        "Examples: None.<br>\n",
        "\n",
        "Features with VIF values between 1 and 5 are generally considered to have moderate multicollinearity. Although there might be some correlation, it is not severe.<br>\n",
        "Examples: \"holiday\", \"workingday\", \"windspeed\", \"pollution\".<br>\n",
        "\n",
        "Features with VIF values above 5 suggest the presence of multicollinearity. These features have a strong correlation with other features in the dataset and may negatively impact the model's performance.<br>\n",
        "Examples: \"temp\", \"atemp\", \"humidity\", \"sunlight\".\n",
        "<br><br>\n",
        "Based on this information, we can consider the VIF values to identify potential issues related to multicollinearity.<br>\n",
        " High VIF values indicate that certain features are highly correlated with others, which can affect the model's interpretability and stability.<br>\n",
        "In such cases, we may consider removing or transforming the highly correlated features to mitigate multicollinearity and improve the model's performance.\n",
        "\n",
        "**The feature \"traffic\" again doing some problems outputting NAN. We will consider removing it because it is behaving abnormally.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5uI9_f0ZPq9"
      },
      "source": [
        "In conclusion, the VIF index helped us to understand in general whether there is a correlation for features.\n",
        "The correlation matrix looked linearly at whether there was a correlation between each 2 features.\n",
        "In both of these indices we saw that the \"temp\", \"atemp\" and \"sunlight\" features has a very high correlation.\n",
        "**All in all, later, we will maybe consider dropping some of them to improve the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G15IFI5TrScB"
      },
      "source": [
        "## Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq6H4A8trzcj"
      },
      "source": [
        "We will check the amount of NULL valuse (We can see that there are no null values):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA1wGX1rrUcs"
      },
      "outputs": [],
      "source": [
        "train_data.isnull().sum().sort_values(ascending = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "413c_-Qy1fQe"
      },
      "source": [
        "There is no need to handle missing values in the pre-process due to no null values as we can see above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rot7m6iisI_v"
      },
      "source": [
        "## Outliers Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to identify potential outliers,\n",
        "lets visualize, as a start, the boxplots of each non-categorial feature:"
      ],
      "metadata": {
        "id": "tUJ7LUuQ4uPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.plot(kind=\"box\",subplots=True,layout=(6,3),figsize=(15,30));"
      ],
      "metadata": {
        "id": "Ce_6SIKS4x3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, from boxplots we can see outliers and the distribution of features.\n",
        "We can see that 'temp', 'atemp', 'humidity', 'windspeed' and 'sunlight' are normally distributed.\n",
        "\n",
        "Also, seems like 'pollution' has a lot of outliers and 'windspeed' has some less outliers.\n",
        "\n",
        "Log transformation compresses the range of the variable. In other words, it brings large values closer together and spreads out small values. This compression can help reduce the influence of extreme values.\n",
        "\n",
        "Also, log transformation can make the distribution of a variable more symmetric. In many cases, taking the logarithm can pull the outliers back towards the center.\n",
        "\n",
        "So, before getting to final conclusions, lets see the log-boxplots of these features:\n"
      ],
      "metadata": {
        "id": "TcI81Lc44zOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting float-type features\n",
        "float_features = train_data.select_dtypes(include=[np.float64, np.float32, np.int64])\n",
        "\n",
        "# Applying logarithmic transformation\n",
        "log_train_data = float_features.apply(np.log1p)\n",
        "\n",
        "# Creating a copy of the original DataFrame\n",
        "transformed_train_data = train_data.copy()\n",
        "\n",
        "# Renaming columns with 'log-' prefix\n",
        "log_feature_names = ['log-' + col for col in log_train_data.columns]\n",
        "log_train_data.columns = log_feature_names\n",
        "\n",
        "# Plotting boxplots with logarithmic values\n",
        "log_train_data.plot(kind=\"box\", subplots=True, layout=(6, 3), figsize=(20, 40))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AnrJUo5g40_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the earlier distribution visualisations and the box plots presented above, it appears that:\n",
        "* **'temp' and 'atemp'** - are normaly distributed.\n",
        "* **'humidity'** - is normaly distributed. Also, 0% and 100% are a bit extreme for values so we need to make sure we clear those out.\n",
        "* **log of 'pollution'** - is close enough to normally distributed.\n",
        "* **'sunlight'** - is normaly distributed.\n",
        "* **'traffic'** - strenghtening our hypothesis that this feature should be removed."
      ],
      "metadata": {
        "id": "0CRRKgou43Fd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final check for normality just to be sure:"
      ],
      "metadata": {
        "id": "tt9IFriH4_cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of rows and columns for the grid\n",
        "num_rows = 3  # Number of rows in the grid\n",
        "num_cols = 2  # Number of columns in the grid\n",
        "\n",
        "# Select specific features from each DataFrame\n",
        "selected_features_train = train_data[['temp', 'atemp', 'humidity', 'sunlight']]\n",
        "selected_features_log_train = log_train_data[['log-pollution']]\n",
        "\n",
        "# Combine the normaly distributed data to one DataFrame\n",
        "ND_data = pd.concat([selected_features_train, selected_features_log_train], axis=1)\n",
        "\n",
        "# Create the grid of subplots\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))\n",
        "\n",
        "# Flatten the axes array for easier indexing\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Iterate over each column in ND_data\n",
        "for i, column in enumerate(ND_data.columns):\n",
        "    # Create Q-Q plot\n",
        "    stats.probplot(ND_data[column], dist=\"norm\", plot=axes[i])\n",
        "\n",
        "    # Set plot title\n",
        "    axes[i].set_title(f\"Q-Q Plot for {column}\")\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g-_ZTDl_5ALC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like indeed our findings were true:\n",
        "\n",
        "* **'temp' and 'atemp'** - are normally distributed.\n",
        "* **'humidity'** - is normally distributed. Also, 0% and 100% are a bit extreme for values so we need to make sure we clear those out.\n",
        "* **'sunlight'** - is normally distributed.\n",
        "* **log of 'pollution'** - is close enough to normally distributed."
      ],
      "metadata": {
        "id": "iipfjWCj5Bss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to find outliers we categorized the features to normaly distributed and not normaly disbributed, By finding the normaly distributed features, we will be able to implement the IQR method for outliers and remove them accordingly later."
      ],
      "metadata": {
        "id": "aUZaukFDgQAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the functions below we will plot the outliers according to the IQR method with several plots and draw conclusions afterwards:"
      ],
      "metadata": {
        "id": "eIU_lHnIm4SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_bounds(df, feature, lower_percentile = 0.25, upper_percentile = 0.75, threshold=1.5):\n",
        "    # Calculate the specified percentiles\n",
        "    p1 = df[feature].quantile(lower_percentile)\n",
        "    p2 = df[feature].quantile(upper_percentile)\n",
        "    spread = p2 - p1\n",
        "\n",
        "    # Define the upper and lower bounds\n",
        "    lower_bound = p1 - threshold * spread\n",
        "    upper_bound = p2 + threshold * spread\n",
        "\n",
        "    # print(f\"For {feature} the threshold given {threshold}, Lower Bound: '{lower_bound}', Upper Bound: '{upper_bound}'.\")\n",
        "    return lower_bound, upper_bound"
      ],
      "metadata": {
        "id": "8Fh370P85DvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_outliers_histogram(df, feature, ax, lower_bound, upper_bound, fontsize=10):\n",
        "    # Plot histogram of the feature\n",
        "    ax.hist(df[feature], bins=20)\n",
        "    ax.set_title(f\"Histogram of {feature}\")\n",
        "    ax.set_xlabel(feature, fontsize=10)\n",
        "    ax.set_ylabel(\"Frequency\", fontsize=10)\n",
        "\n",
        "    # Mark the outliers on the plot\n",
        "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
        "    ax.scatter(outliers[feature], np.zeros_like(outliers[feature]), color='red', marker='x', label='Outliers')\n",
        "\n",
        "    ax.legend()\n",
        "\n",
        "def visualize_outliers_scatter(df, feature, ax, lower_bound, upper_bound, fontsize=10):\n",
        "    # Identify the outliers\n",
        "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
        "\n",
        "    # Visualize the outliers\n",
        "    ax.scatter(df['count'], df[feature], color='blue', label='Data')\n",
        "    ax.scatter(outliers['count'], outliers[feature], color='red', label='Outliers')\n",
        "    ax.set_xlabel('Bike Rental Count', fontsize=10)\n",
        "    ax.set_ylabel(feature, fontsize=10)\n",
        "    ax.set_title(f'Outliers in {feature}')\n",
        "    ax.legend()\n",
        "\n",
        "def visualize_outliers_boxplot(df, feature, ax, lower_bound, upper_bound, fontsize=10):\n",
        "    # Create a boxplot of the feature\n",
        "    sns.boxplot(data=df, y=feature, ax=ax, width=0.5)\n",
        "    ax.set_xlabel(f'{feature}', fontsize=10)\n",
        "    ax.set_ylabel('Values', fontsize=10)\n",
        "    ax.set_title(f'Boxplot of {feature}')\n",
        "\n",
        "\n",
        "def plot_grid_outliers(df, features, plotting_funcs):\n",
        "    num_cols = len(features)\n",
        "    num_funcs = len(plotting_funcs)\n",
        "    rows = num_cols\n",
        "    cols = num_funcs\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(18, 5 * rows))\n",
        "\n",
        "    axes = axes.reshape(-1)  # Reshape axes to a 1D array\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        lower_bound, upper_bound = identify_bounds(df, feature)\n",
        "\n",
        "        for j, plot_func in enumerate(plotting_funcs):\n",
        "\n",
        "            ax = axes[i * num_funcs + j]  # Get the correct axis\n",
        "            plot_func(df, feature, ax, lower_bound, upper_bound)\n",
        "\n",
        "    fig.suptitle('Outlier view for the Normally Distributed Features\\n\\n', fontsize=16, wrap=True)\n",
        "    # plt.figtext(0.5, 0.95, 'Outlier view for the Normally Distributed Features', ha='center', fontsize=16)\n",
        "    plt.subplots_adjust(top=0.9)  # Adjust the top spacing\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "8HNaSD-F5H2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_normal_dist_outliers(IQR_data, IQR_features = ['temp', 'humidity', 'sunlight', 'log-pollution']):\n",
        "\n",
        "    plotting_funcs = [visualize_outliers_histogram, visualize_outliers_scatter, visualize_outliers_boxplot]\n",
        "    plot_grid_outliers(IQR_data, IQR_features, plotting_funcs)"
      ],
      "metadata": {
        "id": "UXw4Cc0X5IDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In each row we plot one of the normally distributed features and mark the outliers."
      ],
      "metadata": {
        "id": "XaQHmRWK5Ifz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ND_data = pd.concat([ND_data, train_labels], axis=1)\n",
        "plot_normal_dist_outliers(ND_data)"
      ],
      "metadata": {
        "id": "-IAREGJP5Iqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like there are not many outliers.<br>\n",
        "Using the IQR method we will remove all the outliers which fall below Q1 – 1.5 IQR or above Q3 + 1.5 IQR.\n",
        "\n",
        "\n",
        "In the preprocessing section we will handle these as decided."
      ],
      "metadata": {
        "id": "3pDLUVi_nNQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not forgetting about the other non-normally distributed, we will handle their outliers using Isolation Forest algorithm in the preprocessing section as well."
      ],
      "metadata": {
        "id": "QchiDES7nNQo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_OOmtAYZnm7"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We will note that:\n",
        "\n",
        "for tree models there is no need to deal with categorical features or data normalization because the tree knows how to deal with it, but it is necessary for a linear regression model and therefore we will perform this procedure for it."
      ],
      "metadata": {
        "id": "EP8nroU-ta7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we would like to make sure that we are making a correct copy of the data that we will go over and make the changes in the pre-processing stage:"
      ],
      "metadata": {
        "id": "W-4XoDnLrk8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_train_data_copy = full_train_data.copy()\n",
        "Y_df = full_train_data_copy['count']\n",
        "X_df = full_train_data_copy.drop('count', axis=1)\n",
        "\n",
        "# Train split to train and validation with fixed random state (42) to ensure reproducibility\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_df, Y_df, test_size=0.2, random_state = 42, shuffle = True)\n",
        "\n",
        "#We will copy the data for trees and data for linear regression models\n",
        "X_train_tree = X_train.copy()\n",
        "Y_train_tree = Y_train.copy()\n",
        "X_train_lr = X_train.copy()\n",
        "Y_train_lr = Y_train.copy()\n",
        "\n",
        "X_val_tree = X_val.copy()\n",
        "Y_val_tree = Y_val.copy()\n",
        "X_val_lr = X_val.copy()\n",
        "Y_val_lr = Y_val.copy()\n",
        "\n",
        "# Save copies in order to have a clean data set to use in the final preprocessing function\n",
        "# (when pipelining all the functions from start to end)\n",
        "X_train_tree_original = X_train.copy()\n",
        "Y_train_tree_original = Y_train.copy()\n",
        "X_val_tree_original = X_val.copy()\n",
        "Y_val_tree_original = Y_val.copy()\n",
        "\n",
        "X_train_lr_original = X_train.copy()\n",
        "Y_train_lr_original = Y_train.copy()\n",
        "X_val_lr_original = X_val.copy()\n",
        "Y_val_lr_original = Y_val.copy()"
      ],
      "metadata": {
        "id": "XtJ9vVDEqKbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96lau3rN1OpT"
      },
      "source": [
        "## Add / Remove Features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we would like to add or delete features according to the conclusions from the EDA:\n",
        "\n",
        "* **Insights from the correlation matrix -** We believed it would be more meaningful to keep temperature in the data and remove atemp and sunlight to enhance the accuracy in capturing the prevailing weather conditions at that specific time of the day. From a temperature feature (temp) we can  learn a lot about the average temperature feature (atemp) and the sunlight feature (sunlight).\n",
        "\n",
        "* **'traffic' -** We would like to remove it from the data because it has a very low correlation with the label. Additionally, it contains unusual numbers whose meaning is difficult to explain, and the overall significance of this feature is unclear.\n",
        "\n",
        "* **'datetime' -** We have split it into its constituent parts, providing all the necessary information for training the model. The datetime type holds no significance in this project; therefore, we opted to break it down into these categorical features: day, month, year, and hour, and remove 'datetime' fom the data.\n",
        "\n",
        "* **'pollution'** - Log transformation brings large values closer together and spreads out small values. This compression can help reduce the influence of outliers. As we saw before in the boxplots, the 'pollution' feature has a lot of outliers, while the boxplot of the log transformation of this feature does not, so the transformation makes it more suitable for enhancing our model training. Consequently, we prefer to exclude 'pollution' and incorporate the logarithm of 'pollution' in our model.\n",
        "\n",
        "TODO - 'Windspeed' feature to log ?!"
      ],
      "metadata": {
        "id": "iFarxPxmriTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adding_new_features(df):\n",
        "\n",
        "  # Adding log of pollution\n",
        "  df['log_pollution'] = np.log1p(df['pollution'])\n",
        "\n",
        "  # Adding day, month, year and hour features (function from EDA)\n",
        "  df = timeseries_engineering(df)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "aFu5yL9tZ_ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpRhPzWpXsAI"
      },
      "outputs": [],
      "source": [
        "def remove_features(df):\n",
        "  # Remove 'atemp' and 'sunlight' variables to address multicollinearity\n",
        "  multicollinearity_features = ['atemp', 'sunlight']\n",
        "  df.drop(multicollinearity_features, axis=1, inplace=True)\n",
        "\n",
        "  #remove traffic\n",
        "  df.drop(['traffic'], axis=1, inplace=True)\n",
        "\n",
        "  #remove datetime\n",
        "  df.drop(['datetime'], axis=1, inplace=True)\n",
        "\n",
        "  #remove pollution\n",
        "  df.drop(['pollution'], axis=1, inplace=True)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering_for_tree(df):\n",
        "  df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "  df = adding_new_features(df)\n",
        "\n",
        "  df = timeseries_features(df)\n",
        "\n",
        "  df = remove_features(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "CrwVrVjKvJjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tree = feature_engineering_for_tree(X_train_tree)"
      ],
      "metadata": {
        "id": "Geb5HerMwihw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_tree = feature_engineering_for_tree(X_val_tree)"
      ],
      "metadata": {
        "id": "A99vBfx5wmvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is relevant to linear regression (in order to deal with categorial features):\n",
        "\n",
        "* Adding and removing features according to our logic:\n",
        "Now we will deal with the features of datetime - If we will keep it as this, we will get too much features."
      ],
      "metadata": {
        "id": "ldCaizwVlLv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing the year into quarters:\n",
        "\n",
        "* **'month'** - Since there are many months (12 months), we would prefer to divide the year into quarters and reduce the number of features. We will add 'time_in_year' feature:\n",
        "  * 1 - 1/4 of the year (1-3 months)\n",
        "  * 2 - 2/4 of the year (4-6 months)\n",
        "  * 3 - 3/4 of the year (7-9 months)\n",
        "  * 4 - 4/4 of the year (10-12 months)"
      ],
      "metadata": {
        "id": "9CvRNJ2TYbMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_time_in_year_feature(month):\n",
        "    if 1 <= month <= 3:\n",
        "        return 1  # 1st quarter\n",
        "    elif 4 <= month <= 6:\n",
        "        return 2  # 2nd quarter\n",
        "    elif 7 <= month <= 9:\n",
        "        return 3  # 3rd quarter\n",
        "    else:\n",
        "        return 4  # 4th quarter"
      ],
      "metadata": {
        "id": "xVfme-xNYaAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing the month into thirds:\n",
        "\n",
        "* **'day'** - Since there are many days (31 days), we would prefer to divide the month into thirds and reduce the number of features. We will add 'time_in_month' feature:\n",
        "  * 1 - Begging of the month (1-10 days)\n",
        "  * 2 - Middle of the month (11-21 days)\n",
        "  * 3 - End of the month (22-31 days)"
      ],
      "metadata": {
        "id": "w3T-85-WYkX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_time_in_month_feature(day):\n",
        "    if 1 <= day <= 10:\n",
        "        return 1  # Beginning of the month\n",
        "    elif 11 <= day <= 21:\n",
        "        return 2  # Middle of the month\n",
        "    else:\n",
        "        return 3  # End of the month"
      ],
      "metadata": {
        "id": "Rc6uC0zAYj9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing the day into times:\n",
        "\n",
        "* **'hour' -** We realized that there are too many hours (24 hours). We would prefer to categorize the data into time ranges: morning, afternoon, and evening. This is done to reduce the number of dimensions.\n",
        "We will add 'time_in_day' feature:\n",
        "  * 1 - Morning\n",
        "  * 2 - Afternoon\n",
        "  * 3 - Evening"
      ],
      "metadata": {
        "id": "qziMFcvj-H6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_time_in_day_feature(hour):\n",
        "  if 4 <= hour < 12:\n",
        "    return 1  # Morning\n",
        "  elif 12 <= hour < 20:\n",
        "    return 2  # Afternoon\n",
        "  else:\n",
        "    return 3  # Evening"
      ],
      "metadata": {
        "id": "hKBSZXA_7j-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply functions and remove: 'month' , 'day' and 'hour':"
      ],
      "metadata": {
        "id": "Brg6R68JfOxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dealing_with_times(df):\n",
        "  # Adding time in year feature\n",
        "  df['time_in_year'] = df['month'].apply(add_time_in_year_feature)\n",
        "  # Adding time in month feature\n",
        "  df['time_in_month'] = df['dayInMonth'].apply(add_time_in_month_feature)\n",
        "  # Adding time in day feature\n",
        "  df['time_in_day'] = df['hour'].apply(add_time_in_day_feature)\n",
        "  # Removing 'month', 'day' and 'hour'\n",
        "  df.drop(['month', 'dayInMonth', 'hour'], axis=1, inplace=True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "t5-PJnnLfB3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO - NEED TO DEL!!!"
      ],
      "metadata": {
        "id": "5yeTVOfgwtg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # TODO Added : change accordingly\n",
        "# def dealing_with_times_v2(df):\n",
        "\n",
        "#   # Adding time in year feature\n",
        "#   df['time_in_year'] = df['month'].apply(add_time_in_year_feature)\n",
        "#   # Adding time in month feature\n",
        "#   df['time_in_month'] = df['dayInMonth'].apply(add_time_in_month_feature)\n",
        "#   # Adding time in day feature\n",
        "#   df['time_in_day'] = df['hour'].apply(add_time_in_day_feature)\n",
        "\n",
        "#   return df"
      ],
      "metadata": {
        "id": "Z58SK1kVaEfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def feature_engineering(df):\n",
        "#   df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "#   df = adding_new_features(df)\n",
        "#   df = dealing_with_times(df)\n",
        "#   df = remove_features(df)\n",
        "#   return df"
      ],
      "metadata": {
        "id": "aRO-iAgu-gRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TODO Added : change accordingly\n",
        "# def feature_engineering_v2(df):\n",
        "#   df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "#   df = adding_new_features(df)\n",
        "\n",
        "#   df = timeseries_features_v2(df)\n",
        "\n",
        "#   df = dealing_with_times_v2(df)\n",
        "\n",
        "#   df = remove_features(df)\n",
        "#   return df"
      ],
      "metadata": {
        "id": "m5J-Qo2HWhR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering_for_linear(df):\n",
        "  df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "  df = adding_new_features(df)\n",
        "\n",
        "  df = dealing_with_times(df)\n",
        "\n",
        "  df = timeseries_features(df)\n",
        "\n",
        "  df = remove_features(df)\n",
        "  return df"
      ],
      "metadata": {
        "id": "K5K43EcJsaJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R8kO0wON8BM"
      },
      "outputs": [],
      "source": [
        "X_train_lr = feature_engineering_for_linear(X_train_lr)\n",
        "X_val_lr = feature_engineering_for_linear(X_val_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMN50RlwN_3U"
      },
      "outputs": [],
      "source": [
        "X_train_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IS7z3wLOAbz"
      },
      "outputs": [],
      "source": [
        "X_val_lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtPSG9CixFck"
      },
      "source": [
        "## Handling Categorial Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the stages before we know which features are categorial and which are not:"
      ],
      "metadata": {
        "id": "w9TEpb-0slw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorial_features = ['year', 'time_in_month', 'time_in_day', 'time_in_year', 'season', 'weather', 'dayInWeek']\n",
        "non_categorial_features = ['holiday', 'workingday', 'temp', 'humidity', 'windspeed', 'log_pollution']"
      ],
      "metadata": {
        "id": "pRUyTVUvsv35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are using the OneHotEncoder to encode the specified categorical features, creating dummy variables for each category."
      ],
      "metadata": {
        "id": "j7yNp576M3cl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUTrRToHxTMr"
      },
      "outputs": [],
      "source": [
        "def OHE_categorial_features(df, categorial_features):\n",
        "  # Create an instance of OneHotEncoder\n",
        "  encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "  # Fit and transform the selected columns\n",
        "  encoded_columns = encoder.fit_transform(df[categorial_features])\n",
        "\n",
        "  # Create custom feature names for the encoded columns\n",
        "  feature_names = []\n",
        "  for i, column in enumerate(categorial_features):\n",
        "      categories = encoder.categories_[i]  # Use encoder.categories_ to get categories\n",
        "      for feature in categories:\n",
        "          feature_names.append(f'{column}_{feature}')\n",
        "\n",
        "  # Create a DataFrame with the encoded columns and custom feature names\n",
        "  encoded_data = pd.DataFrame(encoded_columns, columns=feature_names)\n",
        "  # print(encoded_data.shape)\n",
        "\n",
        "  data_reset = df.reset_index(drop=True)\n",
        "  encoded_data_reset = encoded_data.reset_index(drop=True)\n",
        "\n",
        "  # Concatenate the encoded columns with the original DataFrame\n",
        "  data_encoded = pd.concat([data_reset, encoded_data_reset], axis=1)\n",
        "\n",
        "  df = data_encoded\n",
        "\n",
        "  df.drop(columns=categorial_features, inplace=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr = OHE_categorial_features(X_train, categorial_features)\n",
        "X_val_lr = OHE_categorial_features(X_val, categorial_features)"
      ],
      "metadata": {
        "id": "iEBteefx__iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr.columns"
      ],
      "metadata": {
        "id": "PEZmkgWDAA-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_lr.columns"
      ],
      "metadata": {
        "id": "Xdla9XHGACQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logically: after splitting into times by year, month, day and hour, then:\n",
        "* Day - there are 7 days a week\n",
        "* Month - there are 12 months in a year\n",
        "* Hour - there are 24 round hours in a day\n",
        "* Year - according to the data (the years that appear in the training data and additional years that may appear)\n",
        "\n",
        "Also:\n",
        "* Season and Weather - should be splitted into 4 categories.\n",
        "\n",
        "That's why we have to make sure that all the dummy categories appear in every data set we will work with."
      ],
      "metadata": {
        "id": "8YJjdUDV-Tuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensure_all_categories(df, expected_categories):\n",
        "    # Iterate through each categorical feature\n",
        "    for feature, categories in expected_categories.items():\n",
        "        # Check if the feature is in the DataFrame columns\n",
        "        df_columns_list = list(df.columns)\n",
        "        missing_categories = [cat for cat in categories if cat not in df_columns_list]\n",
        "\n",
        "        # Add missing categories to the DataFrame\n",
        "        if missing_categories:\n",
        "            df = pd.concat([df, pd.DataFrame(0, index=df.index, columns=missing_categories)], axis=1)\n",
        "\n",
        "        # Fill added categories with 0s\n",
        "        df[categories].fillna(0, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "m-NrRZgOc_bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_expected_categories():\n",
        "    expected_categories = {}\n",
        "\n",
        "    # Define categories for 'day'\n",
        "    expected_categories['time_in_year'] = [f'time_in_year_{i}' for i in range(1, 5)]\n",
        "\n",
        "    # Define categories for 'month'\n",
        "    expected_categories['time_in_month'] = [f'time_in_month_{i}' for i in range(1, 4)]\n",
        "\n",
        "    # Define categories for 'hour'\n",
        "    expected_categories['time_in_day'] = [f'time_in_day_{i}' for i in range(1, 4)]\n",
        "\n",
        "    # Define categories for 'year'\n",
        "    expected_categories['year'] = ['year_2011', 'year_2012', 'year_other']\n",
        "\n",
        "    # Define categories for 'season'\n",
        "    expected_categories['season'] = [f'season_{i}' for i in range(1, 5)]\n",
        "\n",
        "    # Define categories for 'weather'\n",
        "    expected_categories['weather'] = [f'weather_{i}' for i in range(1, 5)]\n",
        "\n",
        "    return expected_categories"
      ],
      "metadata": {
        "id": "85zcmXPhO6vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expected_categories = build_expected_categories()\n",
        "X_train_lr = ensure_all_categories(X_train_lr, expected_categories)\n",
        "X_val_lr = ensure_all_categories(X_val_lr, expected_categories)"
      ],
      "metadata": {
        "id": "lwMck5ShFuwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr.columns"
      ],
      "metadata": {
        "id": "5bqQ1LWVQ-LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_lr.columns"
      ],
      "metadata": {
        "id": "FaJDRKn4Q_74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to make sure that the order of the features in every df is the same - we will match our dfs according to the list of the categories we made:"
      ],
      "metadata": {
        "id": "UNjx3l1TR_45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_categorial_features = []\n",
        "for values_list in expected_categories.values():\n",
        "    fixed_categorial_features.extend(values_list)"
      ],
      "metadata": {
        "id": "Tpna6WdpucwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr = X_train_lr[non_categorial_features+fixed_categorial_features]\n",
        "X_val_lr = X_val_lr[non_categorial_features+fixed_categorial_features]"
      ],
      "metadata": {
        "id": "O9_1Qw8WsjBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr.columns"
      ],
      "metadata": {
        "id": "Gv6KVUxeFbIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_lr.columns"
      ],
      "metadata": {
        "id": "GL0gv_IlP3om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a wrapping function that calls all the functions at once:"
      ],
      "metadata": {
        "id": "jM8pLJrwtfnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_categorial_data(df):\n",
        "  categorial_features = ['year', 'time_in_month', 'time_in_day', 'time_in_year', 'season', 'weather', 'dayInWeek']\n",
        "  non_categorial_features = ['holiday', 'workingday', 'temp', 'humidity', 'windspeed', 'log_pollution']\n",
        "\n",
        "  df = OHE_categorial_features(df, categorial_features)\n",
        "\n",
        "  expected_categories = build_expected_categories()\n",
        "\n",
        "  df = ensure_all_categories(df, expected_categories)\n",
        "\n",
        "  fixed_categorial_features = []\n",
        "  for values_list in expected_categories.values():\n",
        "    fixed_categorial_features.extend(values_list)\n",
        "\n",
        "  df = df[non_categorial_features+fixed_categorial_features]\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "xtEVZIFLtnC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxs7JyHhxIoa"
      },
      "source": [
        "##  Outliers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed above, we saw that there are several features that are normally distributed and some that do not.\n",
        "Therfore, we will handle these as follows:\n",
        "\n",
        "- There were a lot of outliers in 'pollution' so we applied log transformation on it and concluded that this feature is normally distributed.\n",
        "- On the <u>Normally distributed</u> which are 'temp', 'humidity' and 'log_pollution' we will use the IQR method to identify them and replace them with the bound they are close to (Upper or Lower).\n",
        "\n",
        "- On the <u>Non-normally distributed feature</u> which is 'windspeed' we will apply an Isolation Forest algorithm by learning on this feature and creating a decision whether a record is labeled as outlier or not. If it does, we will remove it.\n"
      ],
      "metadata": {
        "id": "1KYDZaIfABoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>Normally Distributed Features:</u>"
      ],
      "metadata": {
        "id": "ix9cXZRDoJmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying the IQR Method on the train data - identifying the bounds of the normally distributed features and replacing them with the bounds accordingly:"
      ],
      "metadata": {
        "id": "EjwbpiHBoLOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ND_features = ['temp', 'humidity', 'log_pollution']\n",
        "NND_features = ['windspeed']"
      ],
      "metadata": {
        "id": "9Yr3Oho8oMnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on9F64NLxLdd"
      },
      "outputs": [],
      "source": [
        "def handle_ND_outliers_train(data, ND_features):\n",
        "\n",
        "    # log_features = ['pollution']\n",
        "    # data = apply_log(data, log_features)\n",
        "\n",
        "    # normal_features = ['temp', 'humidity', 'log_pollution']\n",
        "    bounds = {}\n",
        "\n",
        "    for feature in ND_features:\n",
        "        lower_bound, upper_bound = identify_bounds(data, feature)\n",
        "\n",
        "        data[feature] = np.where(data[feature] < lower_bound, lower_bound, data[feature])\n",
        "        data[feature] = np.where(data[feature] > upper_bound, upper_bound, data[feature])\n",
        "\n",
        "        bounds[feature] = (lower_bound, upper_bound)\n",
        "\n",
        "    return data, bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d0I5EXmOgL1"
      },
      "outputs": [],
      "source": [
        "def handle_ND_outliers_test(data, feature_bounds):\n",
        "\n",
        "    # log_features = ['pollution']\n",
        "    # data = apply_log(data, log_features)\n",
        "\n",
        "    for feature, (lower_bound, upper_bound) in feature_bounds.items():\n",
        "        data[feature] = np.where(data[feature] < lower_bound, lower_bound, data[feature])\n",
        "        data[feature] = np.where(data[feature] > upper_bound, upper_bound, data[feature])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>Non-Normally Distributed Features:</u><br>\n"
      ],
      "metadata": {
        "id": "aPaEAaczJ-SR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task, we decided to use Isolation Forest algorithm. <br>\n",
        "This algorithm trains and learns the bounds of each feature and then can be applied on every chosen dataset.\n",
        "Since we want the train data to have the same charactaristics as the test model, we preffered to classify small amount of data as outliers.\n",
        "Also, by doing so, we reduce the chances to overfitting, because the model trains on data that is pretty simillar to the tested one.\n",
        "Therefore we chose the threshold and contamination hyperparameter to be both 0.01. TODO - maybe change the hyperparameter"
      ],
      "metadata": {
        "id": "9BeBE8ATKa6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the function to handle the non-normally distributed features in the training data:"
      ],
      "metadata": {
        "id": "6DA-_slXK8bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_NND_outliers_train(data, labels, threshold, contamination, NND_features):\n",
        "\n",
        "  # Initialize the Isolation Forest model\n",
        "  model = IsolationForest(contamination=contamination)\n",
        "\n",
        "  # NND_features = ['windspeed']\n",
        "  data_selected = data[NND_features].copy()\n",
        "\n",
        "  model.fit(data_selected)\n",
        "\n",
        "  outlier_scores = model.decision_function(data_selected)\n",
        "  outlier_indices = np.where(outlier_scores < threshold)[0]\n",
        "\n",
        "  data.reset_index(drop=True, inplace=True)\n",
        "  labels.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Remove the outlier indices from the data\n",
        "  data = data.drop(outlier_indices)\n",
        "  labels = labels.drop(outlier_indices)\n",
        "\n",
        "  return data, labels"
      ],
      "metadata": {
        "id": "9zkD_m-cK89e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binding both of the decisions together:"
      ],
      "metadata": {
        "id": "WQ_HYLvqLH92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_outliers(X_train, Y_train, X_test):\n",
        "\n",
        "  ND_features = ['temp', 'humidity', 'log_pollution']\n",
        "  NND_features = ['windspeed']\n",
        "\n",
        "  X_train, bounds = handle_ND_outliers_train(X_train, ND_features)\n",
        "  X_test = handle_ND_outliers_test(X_test, bounds)\n",
        "\n",
        "  contamination = 0.001\n",
        "  threshold = 0.001\n",
        "  X_train, Y_train = handle_NND_outliers_train(X_train, Y_train, threshold, contamination, NND_features)\n",
        "\n",
        "  return X_train, Y_train, X_test"
      ],
      "metadata": {
        "id": "zFYTjoIwLIdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before applying the changes, let's see the inital state (Just for the purposes of visualization we will use the data of the tree models only: X_train_tree & X_val_tree because in any case the features of cleaning the outliers are the same for the trees and lr models):"
      ],
      "metadata": {
        "id": "JobpB2AXLZ6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tree.describe()"
      ],
      "metadata": {
        "id": "qZ5d5oyixtnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_tree.describe()"
      ],
      "metadata": {
        "id": "QuQEWVcuxtny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selected_features = ['windspeed', 'log_pollution']\n",
        "df = X_train_tree[NND_features]\n",
        "df.hist()\n",
        "# Add x and y labels\n",
        "plt.xlabel(\"Windspeed\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.title(\"Histogram of NND Features\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rebooW4hLnIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what outliers will be removed in the normally distributed features: (using the plotting function we created in the visualizations stage)"
      ],
      "metadata": {
        "id": "JD-AkCdcL4zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IQR_train = pd.concat([X_train_tree, Y_train_tree], axis=1)\n",
        "# IQR_X_train = X_train[['temp', 'humidity', 'log_pollution']]\n",
        "plot_normal_dist_outliers(IQR_train, ND_features)"
      ],
      "metadata": {
        "id": "D_hsvnMOL8G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the changes:"
      ],
      "metadata": {
        "id": "2NKvkAqnMWFV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnH0u5o7ouYK"
      },
      "outputs": [],
      "source": [
        "X_train_tree, Y_train_tree, X_test_tree = handle_outliers(X_train_tree, Y_train_tree, X_val_lr)\n",
        "X_train_lr, Y_train_lr, X_test_lr = handle_outliers(X_train_lr, Y_train_lr, X_val_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll assess the impact"
      ],
      "metadata": {
        "id": "BgJN05wdL4OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_normal_dist_outliers(pd.concat([X_train_tree, Y_train_tree], axis=1), ND_features)"
      ],
      "metadata": {
        "id": "yPt8JmcyMpMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that the outliers in the normally distributed features were reduced drastically!"
      ],
      "metadata": {
        "id": "xau2TAU-Ms_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the other non-distributed features:"
      ],
      "metadata": {
        "id": "wz44YRlcMu04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = X_train_tree[NND_features]\n",
        "df.hist()\n",
        "# Add x and y labels\n",
        "plt.xlabel(\"Windspeed\", fontsize=12)\n",
        "plt.ylabel(\"Frequency\", fontsize=12)\n",
        "plt.title(\"Histogram of NND Features\", fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6fKv3aTUMxAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tree.describe()"
      ],
      "metadata": {
        "id": "cmy96k1pLznx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_tree.describe()"
      ],
      "metadata": {
        "id": "rPpgaFsmLznx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L_mCifYmzy7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude that indeed the outlier number has been dropped and the changes expected were made."
      ],
      "metadata": {
        "id": "UrBV5yOxNamM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiFHiz6MxXGT"
      },
      "source": [
        "## Data Normalizing\n",
        "\n",
        "this is only relevant to lr model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8LVT-_CxZe_"
      },
      "outputs": [],
      "source": [
        "def fit_normalize_data(df, scaler, col):\n",
        "    df[[col]] = scaler.fit_transform(df[[col]])\n",
        "    return df, scaler\n",
        "\n",
        "def transform_normalize_data(df, scaler, col):\n",
        "    df[[col]] = scaler.transform(df[[col]])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(X_train, X_val):\n",
        "  columns_to_normalize = ['temp', 'humidity', 'windspeed','log_pollution']\n",
        "  scaler = StandardScaler()\n",
        "  for col in columns_to_normalize:\n",
        "    X_train, scaler = fit_normalize_data(X_train, scaler, col)\n",
        "    X_val = transform_normalize_data(X_val, scaler, col)\n",
        "\n",
        "  return X_train, X_val"
      ],
      "metadata": {
        "id": "Zn2z-3MT9Zoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr, X_val_lr = normalize_data(X_train_lr, X_val_lr)"
      ],
      "metadata": {
        "id": "hgmO7LoI_8jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr"
      ],
      "metadata": {
        "id": "gnMCz9jDNmoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_lr"
      ],
      "metadata": {
        "id": "_XI7ytXSNnAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaFEPiItxaqq"
      },
      "source": [
        "## PCA - Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before performing the PCA let's look at the Cumulativ Explained Variance Ratio plot:"
      ],
      "metadata": {
        "id": "1nY7vNgVVQhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variance_vals = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.995, 0.9999]\n",
        "var_dim = []\n",
        "\n",
        "for variance in variance_vals:\n",
        "    pca = PCA(variance)\n",
        "    data = pca.fit_transform(X_train)\n",
        "    print(\"For explained variance:\", variance, \"number of dimensions:\", data.shape[1])\n",
        "    var_dim.append(data.shape[1])\n",
        "\n",
        "plt.plot(variance_vals, var_dim)\n",
        "plt.xlabel(\"Explained Variance\")\n",
        "plt.ylabel(\"Number of Dimensions\")\n",
        "plt.title(\"Number of Dimensions per Explained Variance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KEP_INq5G9UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can evaluate that most of the variance is already gained after approximately 12 dimensions. Therefore, We will choose: threshold=0.95."
      ],
      "metadata": {
        "id": "g4BVoQzHU0df"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6PiIOYlxbfZ"
      },
      "outputs": [],
      "source": [
        "def perform_pca_with_variance(variance_threshold, train_data):\n",
        "    pca = PCA(variance_threshold)\n",
        "    transformed_data = pca.fit_transform(train_data)\n",
        "    return pca, transformed_data\n",
        "\n",
        "def apply_pca_transformation(pca, data):\n",
        "    transformed_data = pca.transform(data)\n",
        "    return transformed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTvPqOy1PCOP"
      },
      "outputs": [],
      "source": [
        "variance_threshold = 0.95\n",
        "\n",
        "pca, X_train_lr_reduced = perform_pca_with_variance(variance_threshold, X_train_lr)\n",
        "X_train_lr_reduced = pd.DataFrame(X_train_lr_reduced)\n",
        "\n",
        "X_val_lr_reduced = apply_pca_transformation(pca, X_val_lr)\n",
        "X_val_lr_reduced = pd.DataFrame(X_val_lr_reduced)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr_reduced"
      ],
      "metadata": {
        "id": "LCdoU76PNLCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_lr_reduced"
      ],
      "metadata": {
        "id": "9qgCGt3i8qg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TXc09TPPJe0"
      },
      "source": [
        "## Final Preprocessing Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4-zmXvRPbzF"
      },
      "source": [
        "Now, after deciding what manipulations we will do on the train data (Outliers removal, Normalization, Missing values handling, Categorial data handling, Feature selection and Feature manipulation), we will create a generic preprocess function to run it all at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yLOPsJpPMrY"
      },
      "outputs": [],
      "source": [
        "def preprocess_data_for_lr(X_train, Y_train, X_test):\n",
        "\n",
        "  # Add & remove features - 'proportion_imports'\n",
        "  X_train = feature_engineering(X_train)\n",
        "  X_test = feature_engineering(X_test)\n",
        "\n",
        "  # Categorical Data Handling\n",
        "  X_train = handle_categorial_data(X_train)\n",
        "  X_test = handle_categorial_data(X_test)\n",
        "\n",
        "  # Outliers Removal\n",
        "  X_train, Y_train, X_test = handle_outliers(X_train, Y_train, X_test)\n",
        "\n",
        "  # Normalization\n",
        "  X_train, X_test = normalize_data(X_train, X_test)\n",
        "\n",
        "  # PCA\n",
        "  pca, X_train = perform_pca_with_variance(variance_threshold, X_train)\n",
        "  X_train = pd.DataFrame(X_train)\n",
        "\n",
        "  X_test = apply_pca_transformation(pca, X_test)\n",
        "  X_test = pd.DataFrame(X_test)\n",
        "\n",
        "  return X_train, Y_train, X_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_for_tree(X_train, Y_train, X_test):\n",
        "\n",
        "  # Add & remove features - 'proportion_imports'\n",
        "  X_train = feature_engineering(X_train)\n",
        "  X_test = feature_engineering(X_test)\n",
        "\n",
        "  # Outliers Removal\n",
        "  X_train, Y_train, X_test = handle_outliers(X_train, Y_train, X_test)\n",
        "\n",
        "  return X_train, Y_train, X_test"
      ],
      "metadata": {
        "id": "wNZH31Gm0vey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr_ppc, Y_train_lr_ppc, X_val_lr_ppc = preprocess_data_for_lr(X_train_lr_original, Y_train_lr_original, X_val_lr_original)"
      ],
      "metadata": {
        "id": "1fRveYsj1Kjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_lr_ppc"
      ],
      "metadata": {
        "id": "Kv_1XGZK1lDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_lr_ppc"
      ],
      "metadata": {
        "id": "uxZT8puy1k5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tree_ppc, Y_train_tree_ppc, X_val_tree_ppc = preprocess_data_for_tree(X_train_tree_original, Y_train_tree_original, X_val_tree_original)"
      ],
      "metadata": {
        "id": "oD4lPLLA1Wt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tree_ppc"
      ],
      "metadata": {
        "id": "LlPnmNwp1jNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_tree_ppc"
      ],
      "metadata": {
        "id": "rt4vYxG41pPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO - NEED TO DEL!!!"
      ],
      "metadata": {
        "id": "LEI8jHD21Fn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # TODO Added : change accordingly\n",
        "# def preprocess_data_v2(X_train, Y_train, X_test):\n",
        "\n",
        "#   # Add & remove features - 'proportion_imports'\n",
        "#   X_train = feature_engineering_v2(X_train)\n",
        "#   X_test = feature_engineering_v2(X_test)\n",
        "\n",
        "#   # Categorical Data Handling\n",
        "#   # X_train = handle_categorial_data(X_train)\n",
        "#   # X_test = handle_categorial_data(X_test)\n",
        "\n",
        "#   # Outliers Removal\n",
        "#   # X_train, Y_train, X_test = handle_outliers(X_train, Y_train, X_test)\n",
        "\n",
        "#   # Normalization\n",
        "#   # X_train, X_test = normalize_data(X_train, X_test)\n",
        "\n",
        "#   return X_train, Y_train, X_test"
      ],
      "metadata": {
        "id": "LL8_DcXvS3H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preproccess_data_with_pca(X_train, Y_train, X_test):\n",
        "\n",
        "#   X_train, Y_train, X_test = preprocess_data(X_train, Y_train, X_test)\n",
        "\n",
        "#   # PCA - Dimensionality Reduction\n",
        "#   pca, X_train = perform_pca_with_variance(variance_threshold, X_train)\n",
        "#   X_train = pd.DataFrame(X_train)\n",
        "\n",
        "#   X_test = apply_pca_transformation(pca, X_test)\n",
        "#   X_test = pd.DataFrame(X_test)\n",
        "\n",
        "#   return X_train, Y_train, X_test"
      ],
      "metadata": {
        "id": "_lOh_7DACR1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJUYWqIaPQ-M"
      },
      "outputs": [],
      "source": [
        "# # Applying all the preprocessing decisions at once from start to end\n",
        "# X_train_ppc, Y_train_ppc, X_val_ppc = preprocess_data(X_train_original, Y_train_original, X_val_original)\n",
        "# # Y_val_original"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_pca_ppc, Y_train_pca_ppc, X_val_pca_ppc = preproccess_data_with_pca(X_train_original, Y_train_original, X_val_original)"
      ],
      "metadata": {
        "id": "JkWMn94p2ziD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_ppc"
      ],
      "metadata": {
        "id": "l1AEI6r2MFMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_val_ppc"
      ],
      "metadata": {
        "id": "6z7j-BxKMH5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_pca_ppc"
      ],
      "metadata": {
        "id": "SI9p0fwrMJ0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_val_pca_ppc"
      ],
      "metadata": {
        "id": "QrP7UZMyMMlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTkmK9sKe32b"
      },
      "source": [
        "# **Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dictionary that consists the 3 algorithms we will use ..."
      ],
      "metadata": {
        "id": "c6-UzyB6ukCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # TODO : this is the working one\n",
        "# X_train_ppc, Y_train_ppc, X_val_ppc = preprocess_data_v2(X_train_original, Y_train_original, X_val_original)"
      ],
      "metadata": {
        "id": "4G44OsR1c5q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_ppc"
      ],
      "metadata": {
        "id": "XpuBNPf8dVaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1c73yiMe32i"
      },
      "outputs": [],
      "source": [
        "models = {'Random forest': None,\n",
        "          'Linear regression': None,\n",
        "          'XGBOOST' : None}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose to use grid search in order to find the best parameters for each model and the parameters that are not mentioned in the Grid search, we took their defult values."
      ],
      "metadata": {
        "id": "kJiyYbGMussJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luG0BykVe32i"
      },
      "source": [
        "## Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHV3kQ-He32j"
      },
      "outputs": [],
      "source": [
        "def run_random_forest(x_train, y_train):\n",
        "  rf = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
        "  tuned_rf = GridSearchCV(estimator=rf,\n",
        "                          param_grid={\n",
        "                              'max_features': ['auto', 'sqrt', 'log2', 1/3],\n",
        "                              'max_depth': [None, 10,30],\n",
        "                              'min_samples_leaf': [1, 2, 4]},\n",
        "                          scoring='neg_root_mean_squared_error',\n",
        "                          cv=3,\n",
        "                          verbose=3,\n",
        "                          refit=True)\n",
        "  tuned_rf.fit(x_train, y_train)\n",
        "  return tuned_rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C2go7Rge32j"
      },
      "outputs": [],
      "source": [
        "#Random Forest\n",
        "#TODO - OLD ONE - TO CHECK IF NEEDED\n",
        "# df = X_train.copy()\n",
        "# valid_df = X_val.copy()\n",
        "# df_labels = Y_train.copy()\n",
        "# valid__df_labels= Y_val.copy()\n",
        "\n",
        "# # Converting 'datetime' Feature\n",
        "# df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "# df['year'] = df['datetime'].dt.year\n",
        "# df['month'] = df['datetime'].dt.month\n",
        "# df['day'] = df['datetime'].dt.day\n",
        "# df['hour'] = df['datetime'].dt.hour\n",
        "\n",
        "# # Drop the original 'datetime' column if needed\n",
        "# df = df.drop('datetime', axis=1)\n",
        "\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#rf_model = RandomForestRegressor(random_state=42)\n",
        "#param_grid = {\n",
        "#    'n_estimators': [100, 1000],\n",
        "#    'max_depth': [None, 10,30],\n",
        "#    'min_samples_split': [2, 5],\n",
        "#    'min_samples_leaf': [1, 2, 4]\n",
        "#}\n",
        "\n",
        "#grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "#grid_search.fit(df, df_labels)\n",
        "\n",
        "#best_params = grid_search.best_params_\n",
        "#print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "#Evaluation\n",
        "#best_rf_model = grid_search.best_estimator_\n",
        "#y_pred = best_rf_model.predict(valid_df)\n",
        "\n",
        "#mse = mean_squared_error(valid__df_labels, y_pred)\n",
        "#rmse = mse**0.5\n",
        "#print(\"Root Mean Squared Error on Test Set:\", rmse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "U_CfSlB8bEqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_linear_regression(x_train, y_train):\n",
        "  lr = LinearRegression()\n",
        "  lr.fit(x_train, np.ravel(y_train))\n",
        "  cv_score = cross_val_score(lr, x_train, y_train, scoring='neg_root_mean_squared_error', cv=3)\n",
        "  return lr, cv_score"
      ],
      "metadata": {
        "id": "lTfAjdXY8ez2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# #Linear Regression Model\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# def lr_model(x_train, y_train):\n",
        "#   lr = LinearRegression()\n",
        "#   lr.fit(x_train,y_train)\n",
        "#   cv_score = cross_val_score(lr, x_train, y_train, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "#   # Convert scores to positive values and calculate the mean\n",
        "#   rmse_score = (-cv_score)**0.5\n",
        "#   mean_rmse = rmse_score.mean()\n",
        "\n",
        "#   print(\"Cross-Validation RMSE Scores:\", rmse_score)\n",
        "#   print(\"Mean Cross-Validation RMSE:\", mean_rmse)\n",
        "#   return lr, cv_score\n"
      ],
      "metadata": {
        "id": "9haMFTH9e32j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "nxYuD62xdzMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#XGboost\n",
        "def run_xgboost(x_train,y_train):\n",
        "  xgb_model = xgb.XGBRegressor(objective='reg:squarederror',n_estimators=1000,random_state=42)\n",
        "  tuned_xgb = GridSearchCV(estimator=xgb_model,\n",
        "                          param_grid={'max_features': ['auto','log2', 1/3],\n",
        "                                      'max_depth' : [3,5,7]},\n",
        "                          scoring='neg_root_mean_squared_error',\n",
        "                          cv=3,\n",
        "                          verbose=3,\n",
        "                          refit=True)\n",
        "  tuned_xgb.fit(x_train, y_train)\n",
        "  return tuned_xgb"
      ],
      "metadata": {
        "id": "i7j6maCRTQdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#possible prameters for xgboost\n",
        "#param_grid = {\n",
        "#    'learning_rate': [0.01, 0.1, 0.2],\n",
        "#    'n_estimators': [50, 100, 200],\n",
        "#    'max_depth': [3, 5, 7],\n",
        "#    'min_child_weight': [1, 3, 5],\n",
        "#    'subsample': [0.7, 0.8, 0.9],\n",
        "#    'gamma': [0, 0.1, 0.2],\n",
        "#    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "#    'alpha': [0, 0.1, 0.5],\n",
        "#    'lambda': [0, 0.1, 0.5]\n",
        "#}"
      ],
      "metadata": {
        "id": "8EKeSUmTe32j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg2jz3-de32k"
      },
      "source": [
        "## Run all models\n",
        "\n",
        "Let's run all models:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tree_models(x, y):\n",
        "  models_dict = {}\n",
        "  models_dict['Random forest'] = run_random_forest(x, y)\n",
        "  models_dict['XGBOOST'] = run_xgboost(x,y)\n",
        "  return models_dict"
      ],
      "metadata": {
        "id": "iqiHnBQZ2bcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = run_all_models(X_train_tree_ppc, Y_train_tree_ppc.values.ravel())"
      ],
      "metadata": {
        "id": "Xe3pOv1Z2oQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models['Linear regression'] = run_linear_regression(_train_lr_ppc, Y_train_lr_ppc)"
      ],
      "metadata": {
        "id": "UtXXgeiQ2pTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "Cb4AajuN2rwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO - NEED TO DEL!"
      ],
      "metadata": {
        "id": "HaXvQup72z_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRueJAGFe32k"
      },
      "outputs": [],
      "source": [
        "# def run_all_models(x, y):\n",
        "#   models_dict = {}\n",
        "#   models_dict['Random forest'] = run_random_forest(x, y)\n",
        "#   models_dict['Linear regression'] = run_linear_regression(x, y)\n",
        "#   models_dict['XGBOOST'] = run_xgboost(x,y)\n",
        "#   return models_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_ppc"
      ],
      "metadata": {
        "id": "fl_xGVRCgjL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgcdkkzZe32k"
      },
      "outputs": [],
      "source": [
        "# models = run_all_models(X_train_ppc, Y_train_ppc.values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvE_fD1ee32k"
      },
      "outputs": [],
      "source": [
        "# models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOMTbWpVe32k"
      },
      "source": [
        "Comparing their cross validation scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKVt5Nsve32k"
      },
      "outputs": [],
      "source": [
        "cv_scores = {}\n",
        "cv_scores['Random forest'] = models['Random forest'].best_score_\n",
        "cv_scores['Linear regression'] = models['Linear regression'][1].mean()\n",
        "cv_scores['XGBOOST'] = models['XGBOOST'].best_score_\n",
        "\n",
        "cv_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at-0GH2he32l"
      },
      "outputs": [],
      "source": [
        "#TODO CHOOSE IF RELEVNT , IF IT DOES - EXPLAIN IT\n",
        "sns.set()\n",
        "\n",
        "\n",
        "cv_scores_df = pd.DataFrame.from_dict(cv_scores, orient='index')\n",
        "cv_scores_df.plot.bar(rot=45, legend=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_scores"
      ],
      "metadata": {
        "id": "IbEseff2HINh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best  Model Parameters"
      ],
      "metadata": {
        "id": "sAIcgh3Btu0m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK3UO6PMe32l"
      },
      "outputs": [],
      "source": [
        "#Finding the best Parameters\n",
        "# parameters found by grid search for RF and XGboost models\n",
        "\n",
        "print(\"Random Forests best parameters are :\", models['Random forest'].best_params_)\n",
        "print(\"xgboost best parameters are:\" ,models['XGBOOST'].best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbvc7vxiZ5fC"
      },
      "source": [
        "# Evaluating on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfQUqyqqZNZN"
      },
      "outputs": [],
      "source": [
        "def evaluate_single_model(x_test, y_test, model):\n",
        "  y_pred = model.predict(x_test)\n",
        "  return np.sqrt(mean_squared_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZh2XxOAaXBt"
      },
      "source": [
        "And test the function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D67k00AXZgPs"
      },
      "outputs": [],
      "source": [
        "evaluate_single_model(X_val_tree_ppc, Y_val_tree_original, models['Random forest'].best_estimator_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4MpmgBYZ78E"
      },
      "source": [
        "Let's create a dictionary that includes only trained models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd-MUc5EaFBM"
      },
      "outputs": [],
      "source": [
        "trained_models_dict = {}\n",
        "trained_models_dict['Random forest'] = models['Random forest'].best_estimator_\n",
        "trained_models_dict['Linear regression'] = models['Linear regression'][0]\n",
        "trained_models_dict['XGBOOST'] = models['XGBOOST'].best_estimator_\n",
        "trained_models_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8kRL7UxanJO"
      },
      "source": [
        "And create a function that iterates over all models: (We added a condition of a PCA that we may do later, Only on the Linear Regression)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0sOG19KZFv1"
      },
      "outputs": [],
      "source": [
        "def evaluate_all_models(x, y, x_lr, y_lr, models_dict):\n",
        "  test_set_scores = {}\n",
        "  for k, v in models_dict.items():\n",
        "    if k == 'Linear regression':\n",
        "      test_set_scores[k] = evaluate_single_model(x_lr, y, v)\n",
        "    else:\n",
        "      test_set_scores[k] = evaluate_single_model(x, y, v)\n",
        "  return test_set_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc14xlYKawF2"
      },
      "source": [
        "Finally, let's run our function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L5oOKaFah_3"
      },
      "outputs": [],
      "source": [
        "test_set_scores = evaluate_all_models(X_val_tree_ppc, Y_val_tree_original, X_val_lr_ppc, Y_val_lr_original, trained_models_dict)\n",
        "test_set_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-dUZ0sAcp9-"
      },
      "source": [
        "### Overall comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuHf2ktQcVGn"
      },
      "source": [
        "Let's combine the two dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F59TlEGHcW4v"
      },
      "outputs": [],
      "source": [
        "combined_dict = {k: [np.abs(v), test_set_scores[k]] for k, v in cv_scores.items()}\n",
        "combined_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-mLRYKNcut1"
      },
      "source": [
        "And compare the CV score to the test set score:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTns2IUka9re"
      },
      "outputs": [],
      "source": [
        "scores_df = pd.DataFrame.from_dict(combined_dict, orient='index', columns=['CV score', 'Test set score'])\n",
        "scores_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection & importance"
      ],
      "metadata": {
        "id": "Y7M2ZqsX6Ofl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can learn and understand which of the models are more or less important and we can delete, add or change any features accordingly."
      ],
      "metadata": {
        "id": "bNPsigUL7Hpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find the importance of all features for each model and remove insignificant features:"
      ],
      "metadata": {
        "id": "QicSMVBF8DIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_dict = {}"
      ],
      "metadata": {
        "id": "JSMtyWFt8Dru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl-WQpBxG49h"
      },
      "source": [
        "#### Tree based methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGXBBs4QGHZX"
      },
      "source": [
        "For tree based models we can use SKLearn's built-in methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tLPrIAzGMYS"
      },
      "outputs": [],
      "source": [
        "def find_tree_feature_importance(model, columns):\n",
        "  importance = model.feature_importances_\n",
        "  importance *= 100 / np.max(importance)  # Normalize\n",
        "  importance = pd.DataFrame(importance, index=columns, columns=[\"Importance\"])\n",
        "  importance = importance.sort_values(by=['Importance'], ascending=False)\n",
        "  return importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2MCkIZDGS6s"
      },
      "outputs": [],
      "source": [
        "feature_importance_dict['Random forest'] = find_tree_feature_importance(trained_models_dict['Random forest'], X_train_tree_ppc.columns)\n",
        "feature_importance_dict['XGBOOST'] = find_tree_feature_importance(trained_models_dict['XGBOOST'], X_train_tree_ppc.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_dict"
      ],
      "metadata": {
        "id": "VgUXhcSSuWqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqYG1lEsG8Aa"
      },
      "source": [
        "#### Classical regression methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIP2pF1pG_lB"
      },
      "source": [
        "One interpetation of feature importance for linear/ridge regression, is the normalized value of the estimator's coefficients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e19JailHJ39"
      },
      "outputs": [],
      "source": [
        "def find_normalized_lr_feature_importance(model, x_train):\n",
        "  coefficients = {x_train.columns[i]: np.abs(model.coef_[i]) for i in range(len(x_train.columns))}\n",
        "  coefficients_df = pd.DataFrame.from_dict(coefficients, orient='index', columns=['Importance'])\n",
        "  coefficients_df['Importance'] *=  x_train.std()\n",
        "  coefficients_df['Importance'] *= 100 / coefficients_df['Importance'].max()\n",
        "  importance = coefficients_df.sort_values(by=['Importance'], ascending=False)\n",
        "  return importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGrXJvRcHMRE"
      },
      "outputs": [],
      "source": [
        "feature_importance_dict['Linear regression'] = find_normalized_lr_feature_importance(trained_models_dict['Linear regression'], X_train_lr_ppc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFVaV87mICFL"
      },
      "source": [
        "#### Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the different feature imprtance across the models:"
      ],
      "metadata": {
        "id": "xZJYFun77odK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set()\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
        "feature_importance_dict['Random forest'].plot(kind='barh', ax=axes[0, 0], legend=False, title='Random forest')\n",
        "feature_importance_dict['XGBOOST'].plot(kind='barh', ax=axes[0, 1], legend=False, title='XGBoost')\n",
        "feature_importance_dict['Linear regression'].plot(kind='barh', ax=axes[1, 0], legend=False, title='Linear regression')"
      ],
      "metadata": {
        "id": "Usv-poL86UNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add/Remove Features to Improve Results"
      ],
      "metadata": {
        "id": "GE5x3tkL9IwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets try to improve our data to get better results (we will use the original dfs and then run the Preproccessing function):\n",
        " - We will remove features according to feature importance methods (comparison)\n"
      ],
      "metadata": {
        "id": "ccMFgK1W8sCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing weak features:\n",
        "\n",
        "* Let's find the 7 weakest features in each model:"
      ],
      "metadata": {
        "id": "YBjNIoYGjBTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weak_features_dict = {}\n",
        "for k, v in feature_importance_dict.items():\n",
        "  weak_features_dict[k] = list(v.index.values[-7:])\n",
        "weak_features_dict"
      ],
      "metadata": {
        "id": "ClJpN8IZjCDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And remove these features from each model. We create a dictionary mapping each model to X_train_ppc and X_test_ppc without the last 3 features:"
      ],
      "metadata": {
        "id": "5keYcMT8jRVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_weakest_features(X_train, X_test):\n",
        "  x_data_dict = {}\n",
        "  for k, v in weak_features_dict.items():\n",
        "    x_data_dict[k] = (X_train.drop(v, axis=1), X_test.drop(v, axis=1))\n",
        "\n",
        "  return X_train, X_test"
      ],
      "metadata": {
        "id": "OKVlT76XjNQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tree_ppc, X_val_tree_ppc = remove_weakest_features(X_train_tree_ppc, X_val_tree_ppc )\n",
        "X_train_lr_ppc, X_val_lr_ppc = remove_weakest_features(X_train_lr_ppc, X_val_lr_ppc)"
      ],
      "metadata": {
        "id": "e6FAyLa9xHaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model improvement"
      ],
      "metadata": {
        "id": "RGzmhm996-4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the models again after the changes:"
      ],
      "metadata": {
        "id": "qIC0aAY79hBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = run_tree_models(X_train_tree_ppc, Y_train_tree_ppc.values.ravel())"
      ],
      "metadata": {
        "id": "DdfxyyM27GqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models['Linear regression'] = run_linear_regression(_train_lr_ppc, Y_train_lr_ppc)"
      ],
      "metadata": {
        "id": "Sj5Ir8Gu4x5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "ETLpjlmc4x5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TODO - NEED TO DEL!!!\n",
        "\n",
        "* We saw that we have a lot of features, so we will decide to perform PCA for the **linear regression**"
      ],
      "metadata": {
        "id": "k_nc9-lI8Pvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# models['Linear regression PCA'] = run_linear_regression(X_train_pca_ppc, Y_train_pca_ppc)"
      ],
      "metadata": {
        "id": "8JEll3z_jvPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "6PJtxPZaj0JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_scores = {}\n",
        "cv_scores['Random forest'] = models['Random forest'].best_score_\n",
        "cv_scores['Linear regression PCA'] = models['Linear regression PCA'][1].mean()\n",
        "cv_scores['Linear regression'] = models['Linear regression'][1].mean()\n",
        "cv_scores['XGBOOST'] = models['XGBOOST'].best_score_"
      ],
      "metadata": {
        "id": "gPrYNup_kHWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_models_dict = {}\n",
        "trained_models_dict['Random forest'] = models['Random forest'].best_estimator_\n",
        "trained_models_dict['Linear regression'] = models['Linear regression'][0]\n",
        "trained_models_dict['Linear regression PCA'] = models['Linear regression PCA'][0]\n",
        "trained_models_dict['XGBOOST'] = models['XGBOOST'].best_estimator_\n",
        "trained_models_dict"
      ],
      "metadata": {
        "id": "EGJsna2hkILQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_set_scores = evaluate_all_models(X_val_ppc, Y_val_original, X_val_pca_ppc, trained_models_dict)\n",
        "# test_set_scores"
      ],
      "metadata": {
        "id": "JZpBRE3akanb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_set_scores = evaluate_all_models(X_val_ppc, Y_val_original, X_val_pca_ppc, trained_models_dict)\n",
        "# test_set_scores"
      ],
      "metadata": {
        "id": "NhxTpcsWklHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_scores = evaluate_all_models(X_val_tree_ppc, Y_val_tree_original, X_val_lr_ppc, Y_val_lr_original, trained_models_dict)\n",
        "test_set_scores"
      ],
      "metadata": {
        "id": "tPsyXeC65J3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dict = {k: [np.abs(v), test_set_scores[k]] for k, v in cv_scores.items()}\n",
        "combined_dict"
      ],
      "metadata": {
        "id": "urHFlUDpkl3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the new:"
      ],
      "metadata": {
        "id": "qKSSthPt93xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_scores_df = pd.DataFrame.from_dict(combined_dict, orient='index', columns=['CV score removed features', 'Test set score removed features'])\n",
        "new_scores_df"
      ],
      "metadata": {
        "id": "DVFz33tzkn7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vystjAzeqzM"
      },
      "source": [
        "And combine to one df:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRryK8fqcY6v"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.concat([scores_df, new_scores_df], axis=1, ignore_index=False)\n",
        "combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's plot the results:\n",
        "\n"
      ],
      "metadata": {
        "id": "RdGjGODIkwDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df[['CV score', 'CV score removed features']].plot(kind='barh', title = 'CV Comparison').legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
      ],
      "metadata": {
        "id": "NMjE54MbkxNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df[['Test set score', 'Test set score removed features']].plot(kind='barh', title = 'Test set score Comparison').legend(loc='center left', bbox_to_anchor=(1.0, 0.5))"
      ],
      "metadata": {
        "id": "Ne098WfvkxfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE4SF0VdaENB"
      },
      "source": [
        "# Prediction - Runing on Test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw that ___ is the best model, this is a tree based model then we will do the preprocessing according to that."
      ],
      "metadata": {
        "id": "SmfXMrKG5anh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy of the test:"
      ],
      "metadata": {
        "id": "yPkU-aX_oiMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT6iVDwwaRM_"
      },
      "outputs": [],
      "source": [
        "X_test_original = test_data.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPC on the test:"
      ],
      "metadata": {
        "id": "s_taJgRwomgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_original = dealing_with_times(X_test_original)"
      ],
      "metadata": {
        "id": "mmyGPfy9ntIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHsHngWKntIp"
      },
      "outputs": [],
      "source": [
        "# Applying all the preprocessing decisions at once from start to end\n",
        "X_train_tree_ppc, Y_train_tree_ppc, X_test_tree_ppc = preprocess_data(X_train_tree_original, Y_train_tree_original, X_test_original)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train_pca_ppc, Y_train_pca_ppc, X_test_pca_ppc = preproccess_data_with_pca(X_train_original, Y_train_original, X_test_original)"
      ],
      "metadata": {
        "id": "oh1qwY2RntIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the best model to find test predictions:"
      ],
      "metadata": {
        "id": "xKwi_dEGop4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Our predictions table: (of our best model XGBoost)\n",
        "y_pred = models['XGBOOST'].best_estimator_.predict(X_test_tree_ppc)\n",
        "y_pred = pd.DataFrame(y_pred,columns = ['Predictions'])\n",
        "y_pred"
      ],
      "metadata": {
        "id": "aG0WY8DYpPzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJrSf6hnWnCJ"
      },
      "source": [
        "# **Output**\n",
        "\n",
        "צריך למלא את זה לפי מה שיצא לנו!!!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s4RrumjWX9q"
      },
      "outputs": [],
      "source": [
        "# Keep keys the same, and replace values according to your results and the specified type\n",
        "\n",
        "results = {'model': ['string1', 'string2', 'string3'],\n",
        "           'Score (RMSE)': ['string1', 'string2', 'string3'],\n",
        "           'Hyperparams used': [['list1'], ['list2'], ['list3']],\n",
        "           'Features dropped': [['list1'], ['list2'], ['list3']],\n",
        "           'New features created': [['list1'], ['list2'], ['list3']],\n",
        "           'Runtime trainining + inference (seconds)': ['int1', 'int2', 'int3'],\n",
        "           'Hardware used (GPU/CPU/TPU)': ['string1', 'string2', 'string3'],\n",
        "           'Explainability (top 3 features)': [['list1'], ['list2'], ['list3']]\n",
        "           }\n",
        "\n",
        "results = pd.DataFrame(results)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCyJPOqeXElz"
      },
      "outputs": [],
      "source": [
        "#Maybe to DEL\n",
        "#results.to_csv(os.path.join(os.getcwd(), 'resultsEx1.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to csv test predictions to the git\n",
        "new_url = 'https://raw.githubusercontent.com/ariel-hedvat/AdvancedMLDLCourseAssignments/main/Assignment%20I/resultsEx1.csv'\n",
        "results.to_csv(new_url, index=False)\n",
        "print(f'The new CSV file has been exported to: {new_url}')"
      ],
      "metadata": {
        "id": "RP5Qq0mgquHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}